{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
   "metadata": {},
   "source": [
    "# Cr√©er son propre mod√®le ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca6836-0007-43f3-af65-d12ae1922c02",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ce code est disponible dans le repo du livre Generative Deep Learning de David Foster disponible [ici](https://github.com/davidADSP/Generative_Deep_Learning_2nd_Edition/blob/main/notebooks/09_transformer/gpt/gpt.ipynb)\n",
    "\n",
    "\n",
    "Vous pouvez √©galement avoir un code disponible [ici](https://keras.io/examples/generative/text_generation_with_miniature_gpt/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea03ee7",
   "metadata": {},
   "source": [
    "# G√©n√©rateur de po√®me\n",
    "\n",
    "\n",
    "\n",
    "## Objectif üéØ\n",
    "\n",
    "- Concevoir et entra√Æner un mod√®le d'intelligence artificielle pour g√©n√©rer des po√®mes d'une taille souhait√©e.\n",
    "\n",
    "## M√©thodologie ‚öôÔ∏è\n",
    "\n",
    "1. **Nettoyage des donn√©es** : √âlimination des doublons, traitement des valeurs manquantes, et pr√©paration des descriptions pour l'entra√Ænement.\n",
    "2. **Entra√Ænement du mod√®le Transformer** : Utilisation d'une architecture Transformer pour apprendre les nuances des descriptions de vin et g√©n√©rer de nouvelles descriptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73350761-bef2-4e96-b3ac-a158eabd2b65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from IPython.display import display, HTML\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
   "metadata": {},
   "source": [
    "## 0. Param√®tres du projet <a name=\"parameters\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d8352af-343e-4c2e-8c91-95f8bac1c8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille du vocabulaire utilis√© pour le mod√®le (nombre de mots uniques pris en compte)\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Longueur maximale des s√©quences trait√©es par le mod√®le\n",
    "MAX_LEN = 80\n",
    "\n",
    "# Dimension de l'embedding des mots (repr√©sentation vectorielle des mots)\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "# Dimension des cl√©s pour l'attention multi-t√™tes\n",
    "KEY_DIM = 256\n",
    "\n",
    "# Nombre de \"t√™tes\" dans l'attention multi-t√™tes\n",
    "N_HEADS = 2\n",
    "\n",
    "# Dimension du r√©seau feed-forward dans le Transformer\n",
    "FEED_FORWARD_DIM = 256\n",
    "\n",
    "# Fraction des donn√©es utilis√©e pour la validation pendant l'entra√Ænement\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Graine al√©atoire pour la reproductibilit√©\n",
    "SEED = 42\n",
    "\n",
    "# Si True, charge un mod√®le pr√©-entra√Æn√© au lieu de former un nouveau mod√®le\n",
    "LOAD_MODEL = False\n",
    "\n",
    "# Taille des lots utilis√©s pendant l'entra√Ænement\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Nombre de cycles complets de passage sur l'ensemble de donn√©es pendant l'entra√Ænement\n",
    "EPOCHS = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7716fac-0010-49b0-b98e-53be2259edde",
   "metadata": {},
   "source": [
    "## 1. Load the data <a name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93cf6b0f-9667-4146-8911-763a8a2925d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\n', '\\n', 'il est temps que je me repose;\\n', 'je suis terrass√© par le sort.\\n', \"ne me parlez pas d'autre chose\\n\", \"que des t√©n√®bres o√π l'on dort!\\n\", '\\n', 'que veut-on que je recommence?\\n', 'je ne demande d√©sormais\\n', '√† la cr√©ation immense\\n', \"qu'un peu de silence et de paix!\\n\", '\\n', \"pourquoi m'appelez-vous encore?\\n\", \"j'ai fait ma t√¢che et mon devoir.\\n\", \"qui travaillait avant l'aurore,\\n\", \"peut s'en aller avant le soir.\\n\", '\\n', '√† vingt ans, deuil et solitude!\\n', 'mes yeux, baiss√©s vers le gazon,\\n', 'perdirent la douce habitude\\n', 'de voir ma m√®re √† la maison.\\n', '\\n', 'elle nous quitta pour la tombe;\\n', \"et vous savez bien qu'aujourd'hui\\n\", 'je cherche, en cette nuit qui tombe,\\n', \"un autre ange qui s'est enfui!\\n\", '\\n', 'vous savez que je d√©sesp√®re,\\n', 'que ma force en vain se d√©fend,\\n', 'et que je souffre comme p√®re,\\n', 'moi qui souffris tant comme enfant!\\n', '\\n', \"mon oeuvre n'est pas termin√©e,\\n\", 'dites-vous. comme adam banni,\\n', 'je regarde ma destin√©e,\\n', \"et je vois bien que j'ai fini.\\n\", '\\n', \"l'humble enfant que dieu m'a ravie\\n\", \"rien qu'en m'aimant savait m'aider;\\n\", \"c'√©tait le bonheur de ma vie\\n\", 'de voir ses yeux me regarder.\\n', '\\n', \"si ce dieu n'a pas voulu clore\\n\", \"l'oeuvre qu'il me fit commencer,\\n\", \"s'il veut que je travaille encore,\\n\", \"il n'avait qu'√† me la laisser!\\n\", '\\n', \"il n'avait qu'√† me laisser vivre\\n\", 'avec ma fille √† mes c√¥t√©s,\\n', \"dans cette extase o√π je m'enivre\\n\", 'de myst√©rieuses clart√©s!\\n', '\\n', \"ces clart√©s, jour d'une autre sph√®re,\\n\", 'o dieu jaloux, tu nous les vends!\\n', \"pourquoi m'as-tu pris la lumi√®re\\n\", \"que j'avais parmi les vivants?\\n\", '\\n', 'as-tu donc pens√©, fatal ma√Ætre,\\n', \"qu'√† force de te contempler,\\n\", 'je ne voyais plus ce doux √™tre,\\n', \"et qu'il pouvait bien s'en aller!\\n\", '\\n', \"t'es-tu dit que l'homme, vaine ombre,\\n\", 'h√©las! perd son humanit√©\\n', 'a trop voir cette splendeur sombre\\n', \"qu'on appelle la v√©rit√©?\\n\", '\\n', \"qu'on peut le frapper sans qu'il souffre,\\n\", \"que son coeur est mort dans l'ennui,\\n\", \"et qu'√† force de voir le gouffre,\\n\", \"il n'a plus qu'un ab√Æme en lui?\\n\", '\\n', \"qu'il va, sto√Øque, o√π tu l'envoies,\\n\", 'et que d√©sormais, endurci,\\n', \"n'ayant plus ici-bas de joies,\\n\", \"il n'a plus de douleurs aussi?\\n\", '\\n', \"as-tu pens√© qu'une √¢me tendre\\n\", \"s'ouvre √† toi pour se mieux fermer,\\n\", 'et que ceux qui veulent comprendre\\n', 'finissent par ne plus aimer?\\n', '\\n', 'o dieu! vraiment, as-tu pu croire\\n', 'que je pr√©f√©rais, sous les cieux,\\n', \"l'effrayant rayon de ta gloire\\n\", 'aux douces lueurs de ses yeux!\\n', '\\n', \"si j'avais su tes lois moroses,\\n\", \"et qu'au m√™me esprit enchant√©\\n\", 'tu ne donnes point ces deux choses,\\n', 'le bonheur et la v√©rit√©,\\n', '\\n', 'plut√¥t que de lever tes voiles,\\n', 'et de chercher, coeur triste et pur,\\n', 'a te voir au fond des √©toiles,\\n', \"o dieu sombre d'un monde obscur,\\n\", '\\n', \"j'eusse aim√© mieux, loin de ta face,\\n\", 'suivre, heureux, un √©troit chemin,\\n']\n"
     ]
    }
   ],
   "source": [
    "# Charger les po√®mes depuis un fichier txt\n",
    "with open(\"poeme_jour13.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    poems = [poem.lower() for poem in file.readlines()]\n",
    "\n",
    "\n",
    "\n",
    "# Visualisation des premiers mots pour v√©rification\n",
    "print(poems[:100])  # Affichage des 1000 premiers mots pour v√©rification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c3f83be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation_and_newlines(s):\n",
    "    # Ajouter de l'espace avant chaque signe de ponctuation\n",
    "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1\", s)\n",
    "    \n",
    "    # Supprimer les caract√®res de ponctuation et les retours √† la ligne\n",
    "    s = re.sub(f\"[{string.punctuation}\\n]\", \"\", s)\n",
    "    \n",
    "    # Supprimer les espaces multiples\n",
    "    s = re.sub(\" +\", \" \", s)\n",
    "    \n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "\n",
    "text_data = [remove_punctuation_and_newlines(x) for x in poems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06b58da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'je cherche en cette nuit qui tombe'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_data = text_data[25]\n",
    "example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c819f285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10971   lignes\n"
     ]
    }
   ],
   "source": [
    "# Comptez les po√®mes\n",
    "n_poems = len(text_data)\n",
    "print(f\"{n_poems}   lignes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f871aaf-d873-41c7-8946-e4eef7ac17c1",
   "metadata": {},
   "source": [
    "##  Tokeniser les donn√©es <a name=\"tokenize\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cabd56",
   "metadata": {},
   "source": [
    "La **tokenisation** est une √©tape fondamentale du traitement du langage naturel (NLP). Elle consiste √† diviser une cha√Æne de texte en unit√©s plus petites, appel√©es \"tokens\". Ces tokens peuvent √™tre aussi simples que des mots ou aussi complexes que des phrases enti√®res.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87d7c65-9a46-492a-a5c0-a043b0d252f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'et que je souffre comme p√®re'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Afficher une description\n",
    "example_data = text_data[30]\n",
    "example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9834f916-b21a-4104-acc9-f28d3bd7a8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convertir en un Dataset TensorFlow\n",
    "text_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(text_data)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "884c0bcb-0807-45a1-8f7e-a32f2c6fa4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une couche de vectorisation\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=\"lower\",                   # Standardiser le texte en le mettant en minuscules\n",
    "    max_tokens=VOCAB_SIZE,                 # Nombre maximal de tokens uniques dans le vocabulaire\n",
    "    output_mode=\"int\",                     # Mode de sortie o√π chaque token est repr√©sent√© par un entier unique\n",
    "    output_sequence_length=MAX_LEN + 1,    # Longueur de sortie pour chaque s√©quence vectoris√©e\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d6dd34a-d905-497b-926a-405380ebcf98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'et',\n",
       " 'l',\n",
       " 'de',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'des',\n",
       " 'que',\n",
       " 'dans',\n",
       " 'qui',\n",
       " '√†',\n",
       " 'est',\n",
       " 'en',\n",
       " 'un',\n",
       " 'd',\n",
       " 'je',\n",
       " 'du',\n",
       " 'qu',\n",
       " 'nous',\n",
       " 'au',\n",
       " 'ce',\n",
       " 'sur',\n",
       " 'il',\n",
       " 'vous',\n",
       " 'j',\n",
       " 's',\n",
       " 'o√π',\n",
       " 'tu',\n",
       " 'une',\n",
       " 'a',\n",
       " 'on',\n",
       " 'aux',\n",
       " 'comme',\n",
       " 'tout',\n",
       " 'se',\n",
       " 'ne',\n",
       " 'pas',\n",
       " 'n',\n",
       " 'pour',\n",
       " 'mon',\n",
       " 'ces',\n",
       " 'ombre',\n",
       " 'plus',\n",
       " 'c',\n",
       " 'sans',\n",
       " 'son',\n",
       " 'dieu',\n",
       " 'sont',\n",
       " 'homme',\n",
       " 'ai',\n",
       " 'quand',\n",
       " 'nuit',\n",
       " 'me',\n",
       " 'ils',\n",
       " 'elle',\n",
       " 'sa',\n",
       " 'tous',\n",
       " 'ma',\n",
       " 'par',\n",
       " '√™tre',\n",
       " 'avec',\n",
       " 'sous',\n",
       " 'mes',\n",
       " 'm',\n",
       " '√¢me',\n",
       " 'ses',\n",
       " 'moi',\n",
       " 'fait',\n",
       " 'cette',\n",
       " 'nos',\n",
       " 'dit',\n",
       " '√¥',\n",
       " 'leur',\n",
       " 'ciel',\n",
       " 'yeux',\n",
       " 'suis',\n",
       " 'si',\n",
       " 'ou',\n",
       " 'bien',\n",
       " 'l√†',\n",
       " 'leurs',\n",
       " 't',\n",
       " 'lui',\n",
       " 'o',\n",
       " 'toi',\n",
       " 'donc',\n",
       " 'jour',\n",
       " 'm√™me',\n",
       " 'amour',\n",
       " 'sombre',\n",
       " 'rien',\n",
       " 'ont',\n",
       " 'mort',\n",
       " 'ton',\n",
       " 'mais',\n",
       " 'vers',\n",
       " '√©tait',\n",
       " 'y',\n",
       " 'vie',\n",
       " 'vent',\n",
       " 'notre',\n",
       " 'dont',\n",
       " 'noir',\n",
       " 'ta',\n",
       " 'terre',\n",
       " 'deux',\n",
       " 'vos',\n",
       " 'coeur',\n",
       " 'autre',\n",
       " 'tes',\n",
       " 'te',\n",
       " 'mal',\n",
       " 'grand',\n",
       " 'fond',\n",
       " 'bas',\n",
       " '¬ª',\n",
       " 'toutes',\n",
       " 'toute',\n",
       " 'cieux',\n",
       " 'esprit',\n",
       " 'voir',\n",
       " 'puis',\n",
       " 'tombe',\n",
       " 'va',\n",
       " 'oeil',\n",
       " 'morts',\n",
       " 'fleurs',\n",
       " 'mains',\n",
       " 'toujours',\n",
       " 'voix',\n",
       " 'peut',\n",
       " 'm√®re',\n",
       " 'voit',\n",
       " 'sort',\n",
       " 'soir',\n",
       " 'seul',\n",
       " 'h√©las',\n",
       " 'doux',\n",
       " 'ceux',\n",
       " 'soleil',\n",
       " 'haut',\n",
       " 'enfant',\n",
       " 'devant',\n",
       " 'heure',\n",
       " 'car',\n",
       " 'pendant',\n",
       " 'ange',\n",
       " 'monde',\n",
       " 'temps',\n",
       " 'front',\n",
       " 'oh',\n",
       " 'jamais',\n",
       " 'hommes',\n",
       " 'gouffre',\n",
       " 'aube',\n",
       " 'votre',\n",
       " 'vont',\n",
       " 'lumi√®re',\n",
       " 'bois',\n",
       " 'ab√Æme',\n",
       " 'triste',\n",
       " 'cet',\n",
       " 'ni',\n",
       " 'font',\n",
       " 'es',\n",
       " 'entre',\n",
       " 'dis',\n",
       " 'bruit',\n",
       " 'azur',\n",
       " 'astres',\n",
       " 'ainsi',\n",
       " 'travers',\n",
       " 'enfants',\n",
       " 'encore',\n",
       " 'vu',\n",
       " 'pr√®s',\n",
       " 'parmi',\n",
       " 'avoir',\n",
       " 'vieux',\n",
       " 'or',\n",
       " 'flots',\n",
       " 'deuil',\n",
       " 'aurore',\n",
       " 'vient',\n",
       " 'noirs',\n",
       " 'loin',\n",
       " 'grands',\n",
       " 'beau',\n",
       " 'immense',\n",
       " 'viens',\n",
       " 'quel',\n",
       " 'peu',\n",
       " 'paris',\n",
       " 'livre',\n",
       " 'ayant',\n",
       " 'sommes',\n",
       " 'porte',\n",
       " 'oiseaux',\n",
       " 'oiseau',\n",
       " 'main',\n",
       " 'infini',\n",
       " '√©toiles',\n",
       " '√¢mes',\n",
       " 't√™te',\n",
       " 't√©n√®bres',\n",
       " 'joie',\n",
       " 'herbe',\n",
       " 'feu',\n",
       " 'eau',\n",
       " 'avez',\n",
       " 'tant',\n",
       " 'r√™ve',\n",
       " 'p√®re',\n",
       " 'puisque',\n",
       " 'mer',\n",
       " 'jours',\n",
       " 'faut',\n",
       " 'douleur',\n",
       " 'pierre',\n",
       " 'non',\n",
       " 'nature',\n",
       " 'clart√©',\n",
       " 'bouche',\n",
       " 'voyant',\n",
       " 'souffle',\n",
       " 'rire',\n",
       " 'regarde',\n",
       " 'plein',\n",
       " 'onde',\n",
       " 'myst√®re',\n",
       " 'monstre',\n",
       " 'fois',\n",
       " 'eux',\n",
       " 'avait',\n",
       " 'air',\n",
       " 'vois',\n",
       " 'vivants',\n",
       " 'sombres',\n",
       " 'femme',\n",
       " 'chaque',\n",
       " '√©tais',\n",
       " 'tombeau',\n",
       " 'regard',\n",
       " 'pleurs',\n",
       " 'matin',\n",
       " 'horreur',\n",
       " 'horizon',\n",
       " '√©toile',\n",
       " 'voil√†',\n",
       " 'sens',\n",
       " 'passe',\n",
       " 'parfois',\n",
       " 'oui',\n",
       " 'maintenant',\n",
       " 'aussi',\n",
       " 'as',\n",
       " 'alors',\n",
       " 'aime',\n",
       " 'aile',\n",
       " 'vents',\n",
       " 'sais',\n",
       " 'quelque',\n",
       " 'pourquoi',\n",
       " 'femmes',\n",
       " 'faire',\n",
       " 'coeurs',\n",
       " 'bras',\n",
       " 'arbre',\n",
       " '√™tres',\n",
       " '√™tes',\n",
       " 'sang',\n",
       " 'sait',\n",
       " 'rayon',\n",
       " 'pieds',\n",
       " 'parce',\n",
       " 'oc√©an',\n",
       " 'obscur',\n",
       " 'fin',\n",
       " 'bon',\n",
       " 'avant',\n",
       " '√©ternit√©',\n",
       " 'univers',\n",
       " 'spectre',\n",
       " 'rit',\n",
       " 'quoi',\n",
       " 'peine',\n",
       " 'jusqu',\n",
       " 'fleur',\n",
       " 'cr√©ation',\n",
       " 'apr√®s',\n",
       " 'aigle',\n",
       " 'ah',\n",
       " '1855',\n",
       " '¬∑',\n",
       " 'voici',\n",
       " 'vis',\n",
       " 'ville',\n",
       " 'veut',\n",
       " 'tour',\n",
       " 'p√¢le',\n",
       " 'lueur',\n",
       " 'gloire',\n",
       " 'fils',\n",
       " 'fais',\n",
       " 'choses',\n",
       " 'veux',\n",
       " 'silence',\n",
       " 'rayons',\n",
       " 'nombre',\n",
       " 'm√™le',\n",
       " 'morne',\n",
       " 'marine',\n",
       " 'corps',\n",
       " 'cheveux',\n",
       " 'astre',\n",
       " '√©t√©',\n",
       " '√©taient',\n",
       " 'vit',\n",
       " 'terrace',\n",
       " 'soit',\n",
       " 'pauvre',\n",
       " 'pass√©',\n",
       " 'mourir',\n",
       " 'maison',\n",
       " 'flot',\n",
       " 'fille',\n",
       " 'dire',\n",
       " 'destin',\n",
       " 'bleu',\n",
       " 'anges',\n",
       " 'sinistre',\n",
       " 'profond',\n",
       " 'parle',\n",
       " 'ouvre',\n",
       " 'moment',\n",
       " 'milieu',\n",
       " 'marche',\n",
       " 'fut',\n",
       " 'doute',\n",
       " 'disait',\n",
       " 'derri√®re',\n",
       " 'debout',\n",
       " 'belle',\n",
       " 'avais',\n",
       " 'autres',\n",
       " 'automne',\n",
       " 'ans',\n",
       " 'ailes',\n",
       " '√©norme',\n",
       " 'songe',\n",
       " 'roses',\n",
       " 'rose',\n",
       " 'pur',\n",
       " 'monte',\n",
       " 'ici',\n",
       " 'genoux',\n",
       " 'fuit',\n",
       " 'foule',\n",
       " 'fosse',\n",
       " 'fit',\n",
       " 'encor',\n",
       " 'devient',\n",
       " 'cris',\n",
       " 'chose',\n",
       " 'chair',\n",
       " 'cendre',\n",
       " 'b√™te',\n",
       " 'bord',\n",
       " 'blanc',\n",
       " 'avons',\n",
       " 'tandis',\n",
       " 'seuil',\n",
       " 'quelqu',\n",
       " 'point',\n",
       " 'mers',\n",
       " 'lune',\n",
       " 'joyeux',\n",
       " 'froid',\n",
       " 'flamme',\n",
       " 'firmament',\n",
       " 'douleurs',\n",
       " 'chacun',\n",
       " 'celui',\n",
       " 'arbres',\n",
       " '√¢pre',\n",
       " 'sent',\n",
       " 'semble',\n",
       " 'roi',\n",
       " 'pris',\n",
       " 'peur',\n",
       " 'petit',\n",
       " 'nuits',\n",
       " 'moins',\n",
       " 'hui',\n",
       " 'entend',\n",
       " 'elles',\n",
       " 'effrayant',\n",
       " 'chemin',\n",
       " 'aujourd',\n",
       " '√©ternelle',\n",
       " '√©coute',\n",
       " 'vivre',\n",
       " 'trop',\n",
       " 'trois',\n",
       " 'souffre',\n",
       " 'raison',\n",
       " 'quatre',\n",
       " 'profonds',\n",
       " 'pleure',\n",
       " 'pleins',\n",
       " 'passant',\n",
       " 'ombres',\n",
       " 'nom',\n",
       " 'meurt',\n",
       " 'mati√®re',\n",
       " 'lentement',\n",
       " 'jeune',\n",
       " 'horrible',\n",
       " 'hiver',\n",
       " 'fleuve',\n",
       " 'flammes',\n",
       " 'c√¥t√©',\n",
       " 'champs',\n",
       " 'calme',\n",
       " 'brume',\n",
       " 'vivant',\n",
       " 'tr√®s',\n",
       " 'souviens',\n",
       " 'songeur',\n",
       " 'septembre',\n",
       " 'saint',\n",
       " 'quelle',\n",
       " 'partout',\n",
       " 'morte',\n",
       " 'malheur',\n",
       " 'lys',\n",
       " 'lieu',\n",
       " 'laisse',\n",
       " 'grande',\n",
       " 'fun√®bres',\n",
       " 'fronts',\n",
       " 'feuilles',\n",
       " 'enfin',\n",
       " 'emplit',\n",
       " 'donne',\n",
       " 'crois',\n",
       " 'autour',\n",
       " '√©ternel',\n",
       " 'voiles',\n",
       " 'vague',\n",
       " 't√™tes',\n",
       " 'sois',\n",
       " 'soeur',\n",
       " 'seigneur',\n",
       " 'rocher',\n",
       " 'prends',\n",
       " 'plume',\n",
       " 'pierres',\n",
       " 'ouragan',\n",
       " 'obscure',\n",
       " 'luit',\n",
       " 'juste',\n",
       " 'histoire',\n",
       " 'heureux',\n",
       " 'goutte',\n",
       " 'for√™t',\n",
       " 'forme',\n",
       " 'd√©j√†',\n",
       " 'douce',\n",
       " 'depuis',\n",
       " 'comment',\n",
       " 'cimeti√®re',\n",
       " 'beaut√©',\n",
       " 'avril',\n",
       " 'affreux',\n",
       " '√©tant',\n",
       " '√©cume',\n",
       " 'vin',\n",
       " 'vais',\n",
       " 'sourire',\n",
       " 'route',\n",
       " 'rois',\n",
       " 'piti√©',\n",
       " 'petite',\n",
       " 'n√©ant',\n",
       " 'nomme',\n",
       " 'noire',\n",
       " 'm√©moire',\n",
       " 'myst√©rieux',\n",
       " 'mot',\n",
       " 'marquis',\n",
       " 'lueurs',\n",
       " 'loi',\n",
       " 'libert√©',\n",
       " 'inconnu',\n",
       " 'immensit√©',\n",
       " 'humaine',\n",
       " 'humain',\n",
       " 'hors',\n",
       " 'gr√¢ce',\n",
       " 'face',\n",
       " 'dormez',\n",
       " 'descend',\n",
       " 'christ',\n",
       " 'blancs',\n",
       " 'blanche',\n",
       " 'amours',\n",
       " 'adieu',\n",
       " 'vol',\n",
       " 'voile',\n",
       " 'vingt',\n",
       " 'vaste',\n",
       " 'vagues',\n",
       " 'souvent',\n",
       " 'sortir',\n",
       " 'solitude',\n",
       " 'sainte',\n",
       " 'reste',\n",
       " 'quelques',\n",
       " 'p√¢les',\n",
       " 'printemps',\n",
       " 'premi√®re',\n",
       " 'pense',\n",
       " 'passer',\n",
       " 'nul',\n",
       " 'mille',\n",
       " 'lit',\n",
       " 'humble',\n",
       " 'fruits',\n",
       " 'farouche',\n",
       " 'doigts',\n",
       " 'demain',\n",
       " 'chante',\n",
       " 'cercueil',\n",
       " 'cela',\n",
       " 'brille',\n",
       " 'branches',\n",
       " 'avenir',\n",
       " 'aller',\n",
       " '1854',\n",
       " 'vision',\n",
       " 'vertu',\n",
       " 'vain',\n",
       " 'tremble',\n",
       " 'terrible',\n",
       " 'soleils',\n",
       " 'sera',\n",
       " 'rhin',\n",
       " 'profonde',\n",
       " 'petits',\n",
       " 'pensif',\n",
       " 'passent',\n",
       " 'orgueil',\n",
       " 'mur',\n",
       " 'montagne',\n",
       " 'mondes',\n",
       " 'moments',\n",
       " 'mai',\n",
       " 'l√®ve',\n",
       " 'longtemps',\n",
       " 'j√©sus',\n",
       " 'jersey',\n",
       " 'jadis',\n",
       " 'haine',\n",
       " 'garde',\n",
       " 'flambeau',\n",
       " 'faites',\n",
       " 'espoir',\n",
       " 'espace',\n",
       " 'd√®s',\n",
       " 'doit',\n",
       " 'doigt',\n",
       " 'dites',\n",
       " 'croit',\n",
       " 'coup',\n",
       " 'chant',\n",
       " 'assez',\n",
       " 'aimer',\n",
       " '¬´je',\n",
       " 'voyons',\n",
       " 'voyez',\n",
       " 'vide',\n",
       " 'ver',\n",
       " 'venir',\n",
       " 'tient',\n",
       " 'savoir',\n",
       " 'puits',\n",
       " 'po√´te',\n",
       " 'peuple',\n",
       " 'pens√©e',\n",
       " 'penche',\n",
       " 'parler',\n",
       " 'nus',\n",
       " 'met',\n",
       " 'lointain',\n",
       " 'jette',\n",
       " 'jardin',\n",
       " 'id√©al',\n",
       " 'hideux',\n",
       " 'formidable',\n",
       " 'exil',\n",
       " 'esprits',\n",
       " 'double',\n",
       " 'disent',\n",
       " 'deuils',\n",
       " 'dessus',\n",
       " 'chien',\n",
       " 'charmant',\n",
       " 'chants',\n",
       " 'cesse',\n",
       " 'cause',\n",
       " 'bl√™me',\n",
       " 'baisers',\n",
       " 'baiser',\n",
       " 'appelle',\n",
       " 'amis',\n",
       " 'voyais',\n",
       " 'vierge',\n",
       " 'verts',\n",
       " 'tombeaux',\n",
       " 's√©pulcre',\n",
       " 'signe',\n",
       " 'seuls',\n",
       " 'rue',\n",
       " 'pri√®re',\n",
       " 'pont',\n",
       " 'pleurer',\n",
       " 'pens√©es',\n",
       " 'paroles',\n",
       " 'obscurit√©',\n",
       " 'nu√©e',\n",
       " 'nu',\n",
       " 'monts',\n",
       " 'mis',\n",
       " 'linceul',\n",
       " 'libre',\n",
       " 'laissant',\n",
       " 'heures',\n",
       " 'grandes',\n",
       " 'fum√©e',\n",
       " 'flotte',\n",
       " 'esp√©rance',\n",
       " 'entends',\n",
       " 'droit',\n",
       " 'dort',\n",
       " 'dieux',\n",
       " 'devoir',\n",
       " 'croire',\n",
       " 'crime',\n",
       " 'cri',\n",
       " 'contre',\n",
       " 'chez',\n",
       " 'celle',\n",
       " 'bonheur',\n",
       " 'beaux',\n",
       " 'aveugle',\n",
       " 'ait',\n",
       " '√©clore',\n",
       " '√©clair',\n",
       " 'viennent',\n",
       " 'vieillard',\n",
       " 'vas',\n",
       " 't√©n√©breux',\n",
       " 'tourne',\n",
       " 'temp√™te',\n",
       " 'sublime',\n",
       " 'souvenir',\n",
       " 'sourd',\n",
       " 'soudain',\n",
       " 'soif',\n",
       " 'semblait',\n",
       " 'savez',\n",
       " 'rome',\n",
       " 'rive',\n",
       " 'prunelle',\n",
       " 'plis',\n",
       " 'plafond',\n",
       " 'pied',\n",
       " 'pauvres',\n",
       " 'paix',\n",
       " 'os',\n",
       " 'nu√©es',\n",
       " 'nuages',\n",
       " 'nid',\n",
       " 'neige',\n",
       " 'm√™mes',\n",
       " 'mortes',\n",
       " 'mord',\n",
       " 'monstrueux',\n",
       " 'mieux',\n",
       " 'mauvais',\n",
       " 'l√®vres',\n",
       " 'lugubre',\n",
       " 'lorsque',\n",
       " 'lois',\n",
       " 'larmes',\n",
       " 'irai',\n",
       " 'invisible',\n",
       " 'instant',\n",
       " 'france',\n",
       " 'fort',\n",
       " 'feux',\n",
       " 'fant√¥me',\n",
       " 'fange',\n",
       " 'e√ªt',\n",
       " 'envie',\n",
       " 'effroi',\n",
       " 'destins',\n",
       " 'dame',\n",
       " 'clart√©s',\n",
       " 'cherche',\n",
       " 'charmants',\n",
       " 'blanches',\n",
       " 'auguste',\n",
       " 'am√®re',\n",
       " '√Æle',\n",
       " '√©glise',\n",
       " '√¢ge',\n",
       " 'vrai',\n",
       " 'voulu',\n",
       " 'voulez',\n",
       " 'voie',\n",
       " 'vil',\n",
       " 'v',\n",
       " 'troupeau',\n",
       " 'tremblant',\n",
       " 'tombent',\n",
       " 'tendre',\n",
       " 'suit',\n",
       " 'solitaire',\n",
       " 'silencieux',\n",
       " 'seule',\n",
       " 'serein',\n",
       " 'sept',\n",
       " 'savent',\n",
       " 'robe',\n",
       " 'regards',\n",
       " 'regarder',\n",
       " 'purs',\n",
       " 'proscrit',\n",
       " 'propre',\n",
       " 'profondeurs',\n",
       " 'probl√®me',\n",
       " 'pri√®res',\n",
       " 'presque',\n",
       " 'prend',\n",
       " 'pousse',\n",
       " 'pourtant',\n",
       " 'plaine',\n",
       " 'paupi√®re',\n",
       " 'passants',\n",
       " 'parfum',\n",
       " 'ouverte',\n",
       " 'nids',\n",
       " 'na√Æt',\n",
       " 'monter',\n",
       " 'mont',\n",
       " 'ma√Ætre',\n",
       " 'marie',\n",
       " 'long',\n",
       " 'jet√©',\n",
       " 'iv',\n",
       " 'ii',\n",
       " 'gr√®ve',\n",
       " 'force',\n",
       " 'faute',\n",
       " 'faisait',\n",
       " 'enfer',\n",
       " 'eaux',\n",
       " 'doucement',\n",
       " 'dessous',\n",
       " 'cr√¢ne',\n",
       " 'crie',\n",
       " 'conna√Æt',\n",
       " 'colombe',\n",
       " 'chansons',\n",
       " 'bleus',\n",
       " 'autrefois',\n",
       " 'aura',\n",
       " 'appara√Ætre',\n",
       " 'angoisse',\n",
       " 'amant',\n",
       " 'allez',\n",
       " 'aim√©',\n",
       " '1846',\n",
       " '√©nigme',\n",
       " '√©claire',\n",
       " '√©chelle',\n",
       " '√©blouissement',\n",
       " 'v√©rit√©',\n",
       " 'v√©nus',\n",
       " 'vo√ªte',\n",
       " 'villes',\n",
       " 'tranquille',\n",
       " 'torture',\n",
       " 'suffit',\n",
       " 'splendeur',\n",
       " 'souffrance',\n",
       " 'souffles',\n",
       " 'sonore',\n",
       " 'sir√®nes',\n",
       " 'sapins',\n",
       " 'sage',\n",
       " 'rochers',\n",
       " 'rend',\n",
       " 'quelquefois',\n",
       " 'pr√™tres',\n",
       " 'pr√™tre',\n",
       " 'profondes',\n",
       " 'portes',\n",
       " 'pleine',\n",
       " 'pla√Æt',\n",
       " 'personne',\n",
       " 'parfums',\n",
       " 'odeur',\n",
       " 'nues',\n",
       " 'nuage',\n",
       " 'murmure',\n",
       " 'mourant',\n",
       " 'maux',\n",
       " 'mars',\n",
       " 'marches',\n",
       " 'marchant',\n",
       " 'luire',\n",
       " 'louis',\n",
       " 'lorsqu',\n",
       " 'longs',\n",
       " 'livide',\n",
       " 'langue',\n",
       " 'j√©hovah',\n",
       " 'juin',\n",
       " 'jean',\n",
       " 'iii',\n",
       " 'humanit√©',\n",
       " 'herbes',\n",
       " 'genre',\n",
       " 'fr√®re',\n",
       " 'frissonner',\n",
       " 'frissonne',\n",
       " 'foudre',\n",
       " 'fatal',\n",
       " 'erreur',\n",
       " 'errer',\n",
       " 'entendre',\n",
       " 'entendons',\n",
       " 'ensemble',\n",
       " 'effet',\n",
       " 'd√©sert',\n",
       " 'drame',\n",
       " 'disant',\n",
       " 'dents',\n",
       " 'croix',\n",
       " 'content',\n",
       " 'constellations',\n",
       " 'connais',\n",
       " 'coin',\n",
       " 'claire',\n",
       " 'clair',\n",
       " 'chute',\n",
       " 'cha√Æne',\n",
       " 'chanter',\n",
       " 'chantent',\n",
       " 'cachot',\n",
       " 'b√©ni',\n",
       " 'bords',\n",
       " 'bons',\n",
       " 'boire',\n",
       " 'beaucoup',\n",
       " 'attends',\n",
       " 'arrive',\n",
       " 'argent',\n",
       " 'appara√Æt',\n",
       " 'allait',\n",
       " 'allais',\n",
       " 'ab√Æmes',\n",
       " 'abord',\n",
       " '√©preuve',\n",
       " '√©l√®ve',\n",
       " '√©cueil',\n",
       " '√©crit',\n",
       " '√©coutez',\n",
       " '√©blouissant',\n",
       " 'vieilles',\n",
       " 'vi',\n",
       " 'verra',\n",
       " 'vermeil',\n",
       " 'venait',\n",
       " 't√¢che',\n",
       " 'tremblants',\n",
       " 'tra√Æne',\n",
       " 'tournant',\n",
       " 'tomber',\n",
       " 'table',\n",
       " 's√©v√®re',\n",
       " 'su',\n",
       " 'source',\n",
       " 'souffrons',\n",
       " 'songes',\n",
       " 'serait',\n",
       " 'seine',\n",
       " 'sages',\n",
       " 'sache',\n",
       " 'r√™veur',\n",
       " 'r√©clame',\n",
       " 'rampe',\n",
       " 'radieux',\n",
       " 'puisqu',\n",
       " 'pr√©sent',\n",
       " 'profondeur',\n",
       " 'prison',\n",
       " 'po√©sie',\n",
       " 'pouvoir',\n",
       " 'plonge',\n",
       " 'pleurent',\n",
       " 'perdu',\n",
       " 'penseurs',\n",
       " 'paupi√®res',\n",
       " 'parts',\n",
       " 'parole',\n",
       " 'parlent',\n",
       " 'pareils',\n",
       " 'page',\n",
       " 'ouvrir',\n",
       " 'ouvrant',\n",
       " 'ouvert',\n",
       " 'nue',\n",
       " 'neuf',\n",
       " 'na√Ætre',\n",
       " 'murs',\n",
       " 'mis√®re',\n",
       " 'miel',\n",
       " 'matelots',\n",
       " 'malheureux',\n",
       " 'malgr√©',\n",
       " 'maisons',\n",
       " 'loup',\n",
       " 'las',\n",
       " 'laisser',\n",
       " 'juillet',\n",
       " 'immobile',\n",
       " 'ignore',\n",
       " 'i',\n",
       " 'hymne',\n",
       " 'hier',\n",
       " 'gronde',\n",
       " 'gouffres',\n",
       " 'globe',\n",
       " 'fr√©missant',\n",
       " 'froids',\n",
       " 'froide',\n",
       " 'frisson',\n",
       " 'fou',\n",
       " 'for√™ts',\n",
       " 'foi',\n",
       " 'flamboie',\n",
       " 'fixe',\n",
       " 'ferme',\n",
       " 'fen√™tre',\n",
       " 'faux',\n",
       " 'fauve',\n",
       " 'famille',\n",
       " 'faite',\n",
       " 'faim',\n",
       " 'faces',\n",
       " 'extase',\n",
       " 'entrer',\n",
       " 'entendais',\n",
       " 'd√©sespoir',\n",
       " 'd√©mons',\n",
       " 'degr√©s',\n",
       " 'dedans',\n",
       " 'danse',\n",
       " 'coule',\n",
       " 'colombes',\n",
       " 'cimes',\n",
       " 'chantant',\n",
       " 'chanson',\n",
       " 'champ',\n",
       " 'caresse',\n",
       " 'b√™tes',\n",
       " 'brise',\n",
       " 'brin',\n",
       " 'blancheur',\n",
       " 'belles',\n",
       " 'autant',\n",
       " 'aurait',\n",
       " 'aucun',\n",
       " 'arri√®re',\n",
       " 'antre',\n",
       " 'ann√©es',\n",
       " 'allons',\n",
       " 'aim√©e',\n",
       " 'affreuse',\n",
       " 'adam',\n",
       " '4',\n",
       " '√©trange',\n",
       " '√©bloui',\n",
       " 'vue',\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adapter la couche aux donn√©es d'entra√Ænement\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "# R√©cup√©rer le vocabulaire g√©n√©r√© par la couche de vectorisation\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6c1c7ce-3cf0-40d4-a3dc-ab7090f69f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "1: [UNK]\n",
      "2: et\n",
      "3: l\n",
      "4: de\n",
      "5: la\n",
      "6: le\n",
      "7: les\n",
      "8: des\n",
      "9: que\n"
     ]
    }
   ],
   "source": [
    "# Afficher quelques correspondances entre tokens et mots\n",
    "for i, word in enumerate(vocab[:10]):\n",
    "    print(f\"{i}: {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8be0a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8961"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cc30186-7ec6-4eb6-b29a-65df6714d321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 17 712  14  70  53  11 124   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Afficher le m√™me exemple converti en entiers\n",
    "example_tokenised = vectorize_layer(example_data)\n",
    "\n",
    "# Afficher la s√©quence d'entiers r√©sultante\n",
    "print(example_tokenised.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c195efb-84c6-4be0-a989-a7542188ad35",
   "metadata": {},
   "source": [
    "## Cr√©er l'ensemble d'entra√Ænement <a name=\"create\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "740294a1-1a6b-4c89-92f2-036d7d1b788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(text):\n",
    "    \"\"\"\n",
    "    Pr√©pare les entr√©es pour l'entra√Ænement en cr√©ant des paires de phrases : \n",
    "    l'une avec le texte original et l'autre avec le texte d√©cal√© d'un mot.\n",
    "    \n",
    "    Args:\n",
    "        text (tf.Tensor): Tensor contenant le texte brut.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Deux tensors, l'un avec le texte original (x) et l'autre avec le texte d√©cal√© d'un mot (y).\n",
    "    \"\"\"\n",
    "    \n",
    "    # √âtendre les dimensions du texte pour le traitement\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    \n",
    "    # Convertir le texte en s√©quences d'entiers\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    \n",
    "    # x contient tous les mots sauf le dernier de chaque phrase\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    \n",
    "    # y contient tous les mots sauf le premier de chaque phrase, d√©calant ainsi les s√©quences d'un mot\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "# Appliquer la fonction `prepare_inputs` √† l'ensemble de donn√©es\n",
    "train_ds = text_ds.map(prepare_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cad80ffb-4298-4249-86b4-9918d62534c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_output = train_ds.take(1).get_single_element()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67ff7263-f62d-44c1-997b-1aa99a393521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
       "array([  21, 8061,    2,   26,   51,   72,  264,    6, 1855,    3,  707,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0])>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exemple\n",
    "example_input_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef2e2cad-414c-4e6d-a2ac-6b9598f9dd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
       "array([8061,    2,   26,   51,   72,  264,    6, 1855,    3,  707,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0])>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple de sortie (d√©cal√© d'un token)\n",
    "example_input_output[1][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff50401-3abe-4c10-bba8-b35bc13ad7d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Cr√©er la fonction de masquage pour l'attention causale <a name=\"causal\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "554a4184-61c2-4eb7-a063-d965586a8188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Cr√©e un masque pour l'attention causale.\n",
    "    \n",
    "    Ce masque permet de s'assurer que lors de la pr√©diction d'un token, \n",
    "    seul le pass√© (les tokens pr√©c√©dents) est pris en compte.\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): Taille du lot.\n",
    "        n_dest (int): Nombre de tokens de destination.\n",
    "        n_src (int): Nombre de tokens source.\n",
    "        dtype (tf.DType): Type des √©l√©ments du masque.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Masque d'attention causale de forme [batch_size, n_dest, n_src].\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cr√©e des indices pour les tokens de destination et source\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    \n",
    "    # D√©termine la relation entre les indices de destination et de source\n",
    "    m = i >= j - n_src + n_dest\n",
    "    \n",
    "    # Convertit le masque bool√©en en type sp√©cifi√©\n",
    "    mask = tf.cast(m, dtype)\n",
    "    \n",
    "    # Redimensionne le masque pour le rendre compatible avec les dimensions attendues\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    \n",
    "    # Duplique le masque pour tout le lot\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "# Affiche le masque d'attention causale transpos√© pour une meilleure visualisation\n",
    "np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501dbad-0860-40ad-b7d6-47950e37858f",
   "metadata": {},
   "source": [
    "## Cr√©er une couche de bloc Transformer<a name=\"transformer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5285a1cb-fce1-46b1-b088-b596002fa9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    Couche de bloc Transformer.\n",
    "    \n",
    "    Ce bloc est compos√© d'une attention multi-t√™tes suivie d'une \n",
    "    normalisation de couche et d'un r√©seau feed-forward.\n",
    "    \n",
    "    Attributes:\n",
    "        num_heads (int): Nombre de t√™tes pour l'attention multi-t√™tes.\n",
    "        key_dim (int): Dimension de la cl√© pour l'attention.\n",
    "        embed_dim (int): Dimension de l'embedding.\n",
    "        ff_dim (int): Dimension du r√©seau feed-forward interne.\n",
    "        dropout_rate (float, optional): Taux de dropout. Par d√©faut √† 0.1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Initialisation des couches\n",
    "        self.attn = layers.MultiHeadAttention(\n",
    "            num_heads, key_dim, output_shape=embed_dim\n",
    "        )\n",
    "        self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
    "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
    "        self.ffn_2 = layers.Dense(self.embed_dim)\n",
    "        self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
    "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Passage en avant du bloc Transformer.\"\"\"\n",
    "        \n",
    "        # Calcul du masque d'attention causale\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(\n",
    "            batch_size, seq_len, seq_len, tf.bool\n",
    "        )\n",
    "\n",
    "        # Application de l'attention multi-t√™tes\n",
    "        attention_output, attention_scores = self.attn(\n",
    "            inputs,\n",
    "            inputs,\n",
    "            attention_mask=causal_mask,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        out1 = self.ln_1(inputs + attention_output)\n",
    "\n",
    "        # Application du r√©seau feed-forward\n",
    "        ffn_1 = self.ffn_1(out1)\n",
    "        ffn_2 = self.ffn_2(ffn_1)\n",
    "        ffn_output = self.dropout_2(ffn_2)\n",
    "        return (self.ln_2(out1 + ffn_output), attention_scores)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Retourne la configuration du bloc Transformer.\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"key_dim\": self.key_dim,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"ff_dim\": self.ff_dim,\n",
    "                \"dropout_rate\": self.dropout_rate,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a6be0-9796-4974-9bcd-6ebbcfe7514e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cr√©er l'Embedding de Tokens et de Position <a name=\"embedder\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdf5cb25-88ae-4026-9e21-c1e6b5094a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Couche d'embedding pour les tokens et les positions.\n",
    "\n",
    "    Cette couche g√©n√®re des embeddings √† la fois pour les tokens (mots ou caract√®res) \n",
    "    et pour leurs positions respectives dans une s√©quence. L'ajout des embeddings de \n",
    "    position est une technique couramment utilis√©e dans des mod√®les comme Transformer.\n",
    "\n",
    "    Attributs:\n",
    "    - max_len: Longueur maximale de la s√©quence.\n",
    "    - vocab_size: Taille du vocabulaire.\n",
    "    - embed_dim: Dimension de l'embedding.\n",
    "    - token_emb: Couche d'embedding pour les tokens.\n",
    "    - pos_emb: Couche d'embedding pour les positions.\n",
    "\n",
    "    M√©thodes:\n",
    "    - call: G√©n√®re les embeddings pour une s√©quence donn√©e.\n",
    "    - get_config: R√©cup√®re la configuration de la couche pour la sauvegarde et la restauration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_len, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Initialise la couche avec les param√®tres donn√©s.\n",
    "        \n",
    "        Param√®tres:\n",
    "        - max_len (int): Longueur maximale de la s√©quence.\n",
    "        - vocab_size (int): Taille du vocabulaire.\n",
    "        - embed_dim (int): Dimension de l'embedding.\n",
    "        \"\"\"\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        # Initialisation de la couche d'embedding pour les tokens\n",
    "        self.token_emb = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        # Initialisation de la couche d'embedding pour les positions\n",
    "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        G√©n√®re les embeddings pour une s√©quence donn√©e.\n",
    "        \n",
    "        Param√®tres:\n",
    "        - x (Tensor): S√©quence d'entr√©e.\n",
    "\n",
    "        Retour:\n",
    "        - Tensor: Embeddings des tokens augment√©s des embeddings de position.\n",
    "        \"\"\"\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        R√©cup√®re la configuration de la couche.\n",
    "\n",
    "        Cette m√©thode est utilis√©e pour la sauvegarde et la restauration de la couche.\n",
    "\n",
    "        Retour:\n",
    "        - dict: Dictionnaire contenant la configuration de la couche.\n",
    "        \"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"max_len\": self.max_len,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2e2d4-5980-47e3-b5b0-6c41c0c2d152",
   "metadata": {},
   "source": [
    "## Construire le mod√®le Transformer <a name=\"transformer_decoder\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c57596e-e17d-4959-b6e8-7581b0bace3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation de l'entr√©e du mod√®le. C'est un tensor d'entiers (indices des tokens) \n",
    "# avec une longueur ind√©finie (d'o√π le 'None'). \n",
    "inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "# Application de la couche d'embedding pour les tokens et les positions.\n",
    "x = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
    "\n",
    "# Application du bloc Transformer, qui renvoie √† la fois la sortie du bloc \n",
    "# et les scores d'attention.\n",
    "x, attention_scores = TransformerBlock(\n",
    "    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM\n",
    ")(x)\n",
    "\n",
    "# La sortie est une densit√© de probabilit√©s sur l'ensemble du vocabulaire \n",
    "# (taille VOCAB_SIZE) pour chaque position de la s√©quence d'entr√©e.\n",
    "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "\n",
    "# Cr√©ation du mod√®le GPT en liant les entr√©es et les sorties d√©finies pr√©c√©demment.\n",
    "gpt = models.Model(inputs=inputs, outputs=[outputs, attention_scores])\n",
    "\n",
    "# Compilation du mod√®le avec l'optimiseur Adam et une fonction de perte \n",
    "# pour la classification multi-classes. La deuxi√®me perte est d√©finie \n",
    "# comme 'None' car nous n'entra√Ænons pas le mod√®le sur les scores d'attention.\n",
    "gpt.compile(\"adam\", loss=[losses.SparseCategoricalCrossentropy(), None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a1c3b0f-3382-444d-bb04-bae143ae5d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, None, 256)         2580480   \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_block (Transfo  ((None, None, 256),       658688    \n",
      " rmerBlock)                   (None, 2, None, None))             \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, None, 10000)       2570000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5809168 (22.16 MB)\n",
      "Trainable params: 5809168 (22.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gpt.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "800a3c6e-fb11-4792-b6bc-9a43a7c977ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    # model.load_weights('./models/model')\n",
    "    gpt = models.load_model(\"./models/gpt\", compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b14665-4359-447b-be58-3fd58ba69084",
   "metadata": {},
   "source": [
    "## Entra√Æner le Transformer <a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ddcff5f-829d-4449-99d2-9a3cb68f7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback pour g√©n√©rer du texte √† la fin de chaque √©poque pendant l'entra√Ænement d'un mod√®le.\n",
    "    \n",
    "    Attributs:\n",
    "        index_to_word (list): Liste des mots, index√©e par leurs indices.\n",
    "        word_to_index (dict): Dictionnaire des mots et de leurs indices correspondants.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, index_to_word, top_k=10):\n",
    "        \"\"\"\n",
    "        Initialise le g√©n√©rateur de texte.\n",
    "        \n",
    "        Args:\n",
    "            index_to_word (list): Liste des mots, index√©e par leurs indices.\n",
    "            top_k (int, optional): Nombre de meilleurs mots √† consid√©rer pour le sampling. Par d√©faut √† 10.\n",
    "        \"\"\"\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {\n",
    "            word: index for index, word in enumerate(index_to_word)\n",
    "        }\n",
    "\n",
    "    def sample_from(self, probs, temperature=1.0):\n",
    "        \"\"\"\n",
    "        √âchantillonne un indice de mot √† partir de probabilit√©s donn√©es.\n",
    "        \n",
    "        Args:\n",
    "            probs (list): Liste des probabilit√©s.\n",
    "            temperature (float): Param√®tre pour contr√¥ler le degr√© d'al√©atoire lors de l'√©chantillonnage.\n",
    "            \n",
    "        Returns:\n",
    "            int: Indice √©chantillonn√©.\n",
    "            list: Probabilit√©s ajust√©es.\n",
    "        \"\"\"\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(len(probs), p=probs), probs\n",
    "\n",
    "    def generate(self, start_prompt, max_tokens, temperature=1.0):\n",
    "        \"\"\"\n",
    "        G√©n√®re un texte √† partir d'un prompt initial.\n",
    "        \n",
    "        Args:\n",
    "            start_prompt (str): Prompt initial pour la g√©n√©ration de texte.\n",
    "            max_tokens (int): Nombre maximal de mots √† g√©n√©rer.\n",
    "            temperature (float): Param√®tre pour contr√¥ler le degr√© d'al√©atoire lors de l'√©chantillonnage.\n",
    "            \n",
    "        Returns:\n",
    "            list: Liste contenant des informations sur chaque mot g√©n√©r√©.\n",
    "        \"\"\"\n",
    "        start_tokens = [\n",
    "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
    "        ]\n",
    "        sample_token = None\n",
    "        info = []\n",
    "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
    "            x = np.array([start_tokens])\n",
    "            y, att = self.model.predict(x, verbose=0)\n",
    "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
    "            info.append(\n",
    "                {\n",
    "                    \"prompt\": start_prompt,\n",
    "                    \"word_probs\": probs,\n",
    "                    \"atts\": att[0, :, -1, :],\n",
    "                }\n",
    "            )\n",
    "            start_tokens.append(sample_token)\n",
    "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
    "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
    "        return info\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        M√©thode appel√©e √† la fin de chaque √©poque pendant l'entra√Ænement.\n",
    "        \n",
    "        Args:\n",
    "            epoch (int): Num√©ro de l'√©poque actuelle.\n",
    "            logs (dict, optional): Dictionnaire des logs d'entra√Ænement.\n",
    "        \"\"\"\n",
    "        self.generate(\"la vie\", max_tokens=80, temperature=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "349865fe-ffbe-450e-97be-043ae1740e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model save checkpoint\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoint/checkpoint.ckpt\",\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "# Tokenize starting prompt\n",
    "text_generator = TextGenerator(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "461c2b3e-b5ae-4def-8bd9-e7bab8c63d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.6775 - dense_2_loss: 0.6775\n",
      "generated text:\n",
      "la vie pour r√©alisant √©blouies on p√™cheurs des spectacle \n",
      "\n",
      "343/343 [==============================] - 57s 164ms/step - loss: 0.6775 - dense_2_loss: 0.6775\n",
      "Epoch 2/10\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3939 - dense_2_loss: 0.3939\n",
      "generated text:\n",
      "la vie entre eux il ne distinguais puis ruisselait les mis \n",
      "\n",
      "343/343 [==============================] - 59s 173ms/step - loss: 0.3939 - dense_2_loss: 0.3939\n",
      "Epoch 3/10\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.3331 - dense_2_loss: 0.3331\n",
      "generated text:\n",
      "la vie est empreinte encore un vague mort l √™tre un fr√©d√©gonde \n",
      "\n",
      "343/343 [==============================] - 60s 175ms/step - loss: 0.3331 - dense_2_loss: 0.3331\n",
      "Epoch 4/10\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2891 - dense_2_loss: 0.2891\n",
      "generated text:\n",
      "la vie est √©clips√© laissant √† moelle et les enfants verrous \n",
      "\n",
      "343/343 [==============================] - 61s 178ms/step - loss: 0.2891 - dense_2_loss: 0.2891\n",
      "Epoch 5/10\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2548 - dense_2_loss: 0.2548\n",
      "generated text:\n",
      "la vie arrive avec ton style ta voix hosties \n",
      "\n",
      "343/343 [==============================] - 56s 165ms/step - loss: 0.2548 - dense_2_loss: 0.2548\n",
      "Epoch 6/10\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.2222 - dense_2_loss: 0.2222\n",
      "generated text:\n",
      "la vie ait ayant pris leur fr√®re avec la gr√¢ce \n",
      "\n",
      "343/343 [==============================] - 56s 163ms/step - loss: 0.2222 - dense_2_loss: 0.2222\n",
      "Epoch 7/10\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1885 - dense_2_loss: 0.1885\n",
      "generated text:\n",
      "la vie √† travers vos bonbons p√™le m√™le \n",
      "\n",
      "343/343 [==============================] - 56s 163ms/step - loss: 0.1885 - dense_2_loss: 0.1885\n",
      "Epoch 8/10\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1583 - dense_2_loss: 0.1583\n",
      "generated text:\n",
      "la vie est compos√©e de deux eaux aventuri√®res \n",
      "\n",
      "343/343 [==============================] - 56s 163ms/step - loss: 0.1583 - dense_2_loss: 0.1583\n",
      "Epoch 9/10\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1354 - dense_2_loss: 0.1354\n",
      "generated text:\n",
      "la vie auguste goutte √† goutte heure par heure \n",
      "\n",
      "343/343 [==============================] - 56s 164ms/step - loss: 0.1354 - dense_2_loss: 0.1354\n",
      "Epoch 10/10\n",
      "343/343 [==============================] - ETA: 0s - loss: 0.1205 - dense_2_loss: 0.1205\n",
      "generated text:\n",
      "la vie auguste goutte √† goutte heure par lui \n",
      "\n",
      "343/343 [==============================] - 56s 163ms/step - loss: 0.1205 - dense_2_loss: 0.1205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x106789ea0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[model_checkpoint_callback, tensorboard_callback, text_generator],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "369bde44-2e39-4bc6-8549-a3a27ecce55c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/gpt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/gpt/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the final model\n",
    "gpt.save(\"./models/gpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e02d2-84dc-40c8-8446-40c09adf1e20",
   "metadata": {},
   "source": [
    "# G√©n√©rer du texte √† l'aide du Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ad23adb-3ec9-4e9a-9a59-b9f9bafca649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_probs(info, vocab, top_k=5):\n",
    "    \"\"\"\n",
    "    Affiche le texte avec une mise en √©vidence bas√©e sur les scores d'attention \n",
    "    et imprime les probabilit√©s des `top_k` mots les plus probables.\n",
    "\n",
    "    Param√®tres:\n",
    "    - info (list) : Liste contenant des informations sur le prompt, les scores d'attention et les probabilit√©s des mots.\n",
    "    - vocab (list) : Liste de mots repr√©sentant le vocabulaire.\n",
    "    - top_k (int) : Nombre de mots les plus probables √† afficher.\n",
    "\n",
    "    \"\"\"\n",
    "    # Pour chaque √©l√©ment dans 'info' (chaque mot g√©n√©r√© et ses d√©tails associ√©s)\n",
    "    for i in info:\n",
    "        highlighted_text = []\n",
    "\n",
    "        # Calculer la mise en √©vidence du texte en fonction des scores d'attention\n",
    "        for word, att_score in zip(\n",
    "            i[\"prompt\"].split(), np.mean(i[\"atts\"], axis=0)\n",
    "        ):\n",
    "            highlighted_text.append(\n",
    "                # Cr√©e une mise en √©vidence bas√©e sur le score d'attention pour le mot actuel\n",
    "                '<span style=\"background-color:rgba(135,206,250,'\n",
    "                + str(att_score / max(np.mean(i[\"atts\"], axis=0)))\n",
    "                + ');\">'\n",
    "                + word\n",
    "                + \"</span>\"\n",
    "            )\n",
    "        highlighted_text = \" \".join(highlighted_text)\n",
    "        \n",
    "        # Affiche le texte mis en √©vidence\n",
    "        display(HTML(highlighted_text))\n",
    "\n",
    "        # Obtenir les probabilit√©s du mot g√©n√©r√©\n",
    "        word_probs = i[\"word_probs\"]\n",
    "        \n",
    "        # Obtenir les indices et les probabilit√©s des `top_k` mots les plus probables\n",
    "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
    "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
    "        \n",
    "        # Afficher chaque mot et sa probabilit√© associ√©e\n",
    "        for p, i in zip(p_sorted, i_sorted):\n",
    "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
    "        \n",
    "        # Imprimer une ligne de s√©paration\n",
    "        print(\"--------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cf25578-d47c-4b26-8252-fcdf2316a4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text:\n",
      "le ciel s emplit alors de millions d hirondelles \n",
      "\n"
     ]
    }
   ],
   "source": [
    "info = text_generator.generate(\n",
    "    \"le ciel\", max_tokens=1000, temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ae2da8e-9b7c-4b71-b37b-021115b3d7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text:\n",
      "Un matin je me prom√®ne \n",
      "\n"
     ]
    }
   ],
   "source": [
    "info = text_generator.generate(\n",
    "    \"Un matin\", max_tokens=2000, temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cae6d5d-263d-4455-b96c-f315cbe284ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextGenerator' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mtext_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mla\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m print_probs(info, vocab)\n",
      "Cell \u001b[0;32mIn[24], line 58\u001b[0m, in \u001b[0;36mTextGenerator.generate\u001b[0;34m(self, start_prompt, max_tokens, temperature)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(start_tokens) \u001b[38;5;241m<\u001b[39m max_tokens \u001b[38;5;129;01mand\u001b[39;00m sample_token \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     57\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([start_tokens])\n\u001b[0;32m---> 58\u001b[0m     y, att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(x, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     59\u001b[0m     sample_token, probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_from(y[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], temperature)\n\u001b[1;32m     60\u001b[0m     info\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     61\u001b[0m         {\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: start_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m         }\n\u001b[1;32m     66\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TextGenerator' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "info = text_generator.generate(\n",
    "    \"la\", max_tokens=80, temperature=0.5\n",
    ")\n",
    "print_probs(info, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb86e254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763d958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
