{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "391c5233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (4.36.0.dev0)\n",
      "Requirement already satisfied: filelock in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n"
     ]
    }
   ],
   "source": [
    "# Installation de la bibliothèque transformers de Hugging Face\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8addf8",
   "metadata": {},
   "source": [
    "# Analyse des sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce283a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFCamembertForSequenceClassification.\n",
      "\n",
      "All the layers of TFCamembertForSequenceClassification were initialized from the model checkpoint at tblard/tf-allocine.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFCamembertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9828377962112427}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Pipeline pour l'analyse de sentiments\n",
    "analyse_sentiments = pipeline('sentiment-analysis', model=\"tblard/tf-allocine\")\n",
    "\n",
    "# Exemple d'analyse de sentiment\n",
    "analyse_sentiments(\"J'aime beaucoup LeCoinStat!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0c477",
   "metadata": {},
   "source": [
    "# Traduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0d546ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'apprends au cours du défi #30daysgenerativeai.\n"
     ]
    }
   ],
   "source": [
    "# Pipeline pour la traduction (Anglais vers Français par exemple)\n",
    "traducteur = pipeline('translation_en_to_fr', model='t5-base')\n",
    "\n",
    "# Traduction d'une phrase de l'anglais vers le français\n",
    "phrase_traduite = traducteur(\"I am learning during the #30daysgenerativeai challenge.\")\n",
    "\n",
    "# Affichage de la phrase traduite\n",
    "print(phrase_traduite[0]['translation_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c5d30",
   "metadata": {},
   "source": [
    "# Generation de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ccef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le Challenge #30daysgenerativeai c'est pas pour tout de suite, mais je suis pas sûr que ça soit la bonne solution. Le premier est un peu plus long, mais il est très agréable à lire. Il est donc important de bien choisir son matériel. Il est donc important de bien choisir son matériel de ski. Il est donc important de bien choisir son matériel de ski. Il est donc important de bien choisir son matériel de ski. Il est donc important de bien choisir son matériel de ski. Le premier est un peu plus long, mais il est très agréable à lire. Il est donc important de bien choisir son matériel de ski. Le premier est un peu plus long, mais il est très agréable à lire. Il est donc important de bien choisir son matériel. Le premier est un peu plus long, mais il est très agréable à lire. Il est donc important de bien choisir son matériel. Le premier est un peu plus long, mais il est très agréable à lire\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Chargement du tokenizer et du modèle pour la génération de texte\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"antoiloui/belgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"antoiloui/belgpt2\")\n",
    "\n",
    "# Préparation des entrées pour la génération de texte, avec attention_mask\n",
    "inputs = tokenizer(\"Le Challenge #30daysgenerativeai c'est\", return_tensors=\"pt\")\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Génération de texte en français en fournissant l'attention_mask et en définissant pad_token_id si nécessaire\n",
    "text_generation = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id  # Définir si nécessaire\n",
    ")\n",
    "\n",
    "# Affichage du texte généré\n",
    "print(tokenizer.decode(text_generation[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee758fc7",
   "metadata": {},
   "source": [
    "# Classification zeto-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db167f",
   "metadata": {},
   "source": [
    "La classification à zero-shot fait référence à la capacité d'un modèle à classer correctement des textes dans des catégories sans avoir reçu d'exemples spécifiques lors de son entraînement. \n",
    "\n",
    "Le modèle utilise sa compréhension générale de la langue pour faire des hypothèses sur la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7451a00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"Cet article traite de l'importance de l'intelligence artificielle dans la société moderne.\",\n",
       " 'labels': ['technologie', 'politique', 'éducation', 'économie'],\n",
       " 'scores': [0.724689781665802,\n",
       "  0.09451793879270554,\n",
       "  0.09433780610561371,\n",
       "  0.08645441383123398]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Pipeline pour la classification à zéro coup\n",
    "classify_zero_shot = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Exemple de classification à zéro coup en français\n",
    "classify_zero_shot(\n",
    "    \"Cet article traite de l'importance de l'intelligence artificielle dans la société moderne.\",\n",
    "    candidate_labels=[\"éducation\", \"politique\", \"technologie\", \"économie\"],\n",
    "    hypothesis_template=\"Cet article est sur {}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beddf3c7",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b1bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Je', 'suis', 'ra', '##vi', 'de', 'découvrir', 'des', 'tuto', '##rie', '##ls', 'sur', 'l', \"'\", 'intelligence', 'arti', '##ficie', '##lle', '.']\n",
      "Identifiants de Tokens: [101, 13796, 49301, 11859, 11310, 10104, 91134, 10139, 69635, 12904, 11747, 10326, 180, 112, 30151, 46118, 72138, 11270, 119, 102]\n",
      "Résultat de l'analyse de sentiment: [{'label': 'LABEL_1', 'score': 0.6690822243690491}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Nom du modèle pré-entraîné que vous souhaitez utiliser\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "# Chargement du tokenizer et du modèle pour la classification de séquences\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Création d'une pipeline de classification de sentiment\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Exemple de texte à analyser\n",
    "texte = \"Je suis ravi de découvrir des tutoriels sur l'intelligence artificielle.\"\n",
    "\n",
    "# Tokenisation du texte\n",
    "tokens = tokenizer.tokenize(texte)\n",
    "input_ids = tokenizer.encode(texte)\n",
    "\n",
    "# Affichage des tokens et des identifiants de tokens\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Identifiants de Tokens:\", input_ids)\n",
    "\n",
    "# Exécution de la pipeline de classification de sentiment sur le texte\n",
    "resultat = classifier(texte)\n",
    "\n",
    "# Affichage du résultat de l'analyse de sentiment\n",
    "print(\"Résultat de l'analyse de sentiment:\", resultat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23de2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Je', 'suis', 'ra', '##vi', 'de', 'découvrir', 'des', 'tuto', '##rie', '##ls', 'sur', 'l', \"'\", 'intelligence', 'arti', '##ficie', '##lle', '.']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Nom du modèle pré-entraîné que vous souhaitez utiliser\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "\n",
    "# Chargement du tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Exemple de texte à tokeniser\n",
    "sequence = \"Je suis ravi de découvrir des tutoriels sur l'intelligence artificielle.\"\n",
    "\n",
    "# Tokenisation de la séquence\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08709a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifiants de Tokens: [13796, 49301, 11859, 11310, 10104, 91134, 10139, 69635, 12904, 11747, 10326, 180, 112, 30151, 46118, 72138, 11270, 119]\n"
     ]
    }
   ],
   "source": [
    "# Conversion des tokens en identifiants de tokens\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Identifiants de Tokens:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91da6715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Séquence décodée: Je suis ravi de découvrir des tutoriels sur l'intelligence artificielle.\n"
     ]
    }
   ],
   "source": [
    "# Décodage des identifiants de tokens pour récupérer la séquence\n",
    "decoded_sequence = tokenizer.decode(token_ids)\n",
    "print(\"Séquence décodée:\", decoded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477f305",
   "metadata": {},
   "source": [
    "# Faire un résumé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b3eb4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'intelligence artificielle est un domaine de l'informatique. Les algorithmes d'apprentissage automatique permettent aux ordinateurs de s'entraîner sur des donn\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Charger la pipeline de résumé avec le modèle BART\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Exemple de texte long à résumer\n",
    "text = \"\"\"\n",
    "L'intelligence artificielle est un domaine de l'informatique qui met l'accent sur la création de machines capables de travailler et de réagir comme des humains. Certains des exemples de travail dans ce domaine sont l'apprentissage automatique, où les ordinateurs, les logiciels et les appareils effectuent via des algorithmes des tâches de manière intelligente. Les algorithmes d'apprentissage automatique, qui sont au cœur de l'intelligence artificielle, permettent aux ordinateurs de s'entraîner sur des données fournies puis d'utiliser ces données pour prédire et prendre des décisions basées sur de nouvelles données. Les avantages de l'intelligence artificielle sont nombreux et peuvent avoir un impact significatif sur les secteurs où la précision et la cohérence sont cruciales.\n",
    "\"\"\"\n",
    "\n",
    "# Effectuer le résumé\n",
    "summary = summarizer(text, max_length=50, min_length=30, do_sample=False)\n",
    "\n",
    "# Afficher le résumé\n",
    "print(summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7833f",
   "metadata": {},
   "source": [
    "# Générer une description de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac2c25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n",
    "def generer_legende_en_francais(chemin_image):\n",
    "    \"\"\"\n",
    "    Génère une légende pour une image donnée en anglais et la traduit en français.\n",
    "\n",
    "    :param chemin_image: Le chemin vers l'image pour laquelle générer une légende.\n",
    "    :type chemin_image: str\n",
    "    :return: La légende traduite en français.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialiser la pipeline de légendage d'image\n",
    "    pipeline_legende_image = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "    # Initialiser la pipeline de traduction de l'anglais vers le français\n",
    "    pipeline_traduction = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "    \n",
    "    # Génération de la légende en anglais\n",
    "    resultats_legende = pipeline_legende_image(chemin_image)\n",
    "    legende_anglaise = resultats_legende[0]['generated_text']\n",
    "\n",
    "    # Traduction de la légende en français\n",
    "    legende_francaise = pipeline_traduction(legende_anglaise, max_length=512)\n",
    "    texte_francais = legende_francaise[0]['translation_text']\n",
    "\n",
    "    return texte_francais\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7958391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "/Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il y a une femme debout à côté d'un panneau sur le côté de la route\n"
     ]
    }
   ],
   "source": [
    "# Exemple d'appel de la fonction\n",
    "\n",
    "legende_francaise = generer_legende_en_francais(\"photo.jpg\")\n",
    "print(legende_francaise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d0790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
