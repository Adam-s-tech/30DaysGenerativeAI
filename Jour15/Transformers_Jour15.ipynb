{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca44c0d8",
   "metadata": {},
   "source": [
    "# Comprendre les mécanismes d'attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4341dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dcfd1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de l'embedding de la phrase: (1, 4, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialisation\n",
    "sentence = \"Quel temps fait-il\"\n",
    "embedding_layer = tf.keras.layers.Embedding(5000, 256)  \n",
    "tokenized_sentence = [15, 120, 260, 45]  # Tokenisation arbitraire pour l'exemple\n",
    "embedded_sentence = embedding_layer(tf.convert_to_tensor([tokenized_sentence]))\n",
    "print(\"Dimension de l'embedding de la phrase:\", embedded_sentence.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14da3653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encodeur:\n",
      "Dimension de Q: (1, 4, 256)\n",
      "Dimension de K: (1, 4, 256)\n",
      "Dimension de V: (1, 4, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encodeur\n",
    "## Calcul des Query, Key, Value\n",
    "Q_enc = tf.keras.layers.Dense(256)(embedded_sentence)\n",
    "K_enc = tf.keras.layers.Dense(256)(embedded_sentence)\n",
    "V_enc = tf.keras.layers.Dense(256)(embedded_sentence)\n",
    "print(\"\\nEncodeur:\")\n",
    "print(\"Dimension de Q:\", Q_enc.shape)\n",
    "print(\"Dimension de K:\", K_enc.shape)\n",
    "print(\"Dimension de V:\", V_enc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec6df7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de l'attention: (1, 4, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Attention\n",
    "QK_enc = tf.matmul(Q_enc, K_enc, transpose_b=True)\n",
    "QK_normalized_enc = QK_enc / tf.math.sqrt(tf.cast(256, tf.float32))\n",
    "softmax_enc = tf.nn.softmax(QK_normalized_enc)\n",
    "attention_enc = tf.matmul(softmax_enc, V_enc)\n",
    "print(\"Dimension de l'attention:\", attention_enc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac016984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Décodeur:\n",
      "Dimension de Q (décodeur): (1, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Décodeur (pour prédire le prochain mot)\n",
    "## Utilisez l'état du dernier mot comme Query\n",
    "Q_dec = Q_enc[:, -1, :]\n",
    "print(\"\\nDécodeur:\")\n",
    "print(\"Dimension de Q (décodeur):\", Q_dec.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e14a4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de l'attention (décodeur): (1, 1, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Attention\n",
    "QK_dec = tf.matmul(tf.expand_dims(Q_dec, 1), K_enc, transpose_b=True)\n",
    "QK_normalized_dec = QK_dec / tf.math.sqrt(tf.cast(256, tf.float32))\n",
    "softmax_dec = tf.nn.softmax(QK_normalized_dec)\n",
    "attention_dec = tf.matmul(softmax_dec, V_enc)\n",
    "print(\"Dimension de l'attention (décodeur):\", attention_dec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c33b8054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimension de la sortie: (1, 1, 5000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sortie: prédiction du prochain mot\n",
    "output = tf.keras.layers.Dense(5000, activation='softmax')(attention_dec)  \n",
    "print(\"\\nDimension de la sortie:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f67542f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index du mot prédit: [[1558]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prédire le mot avec l'indice le plus élevé comme mot suivant\n",
    "predicted_next_word_index = tf.argmax(output, axis=-1)\n",
    "print(\"Index du mot prédit:\", predicted_next_word_index.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100d2db6",
   "metadata": {},
   "source": [
    "# Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ae0456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f08eaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, num_heads):\n",
    "    \"\"\"Divise les dernières dimensions de x en (num_heads, depth).\"\"\"\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    d_model = x.shape[-1]\n",
    "    depth = d_model // num_heads\n",
    "\n",
    "    reshaped_x = tf.reshape(x, (batch_size, -1, num_heads, depth))\n",
    "    return tf.transpose(reshaped_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "def multi_head_attention(Q, K, V, num_heads):\n",
    "    \"\"\"Implémentation de la multi-head attention.\"\"\"\n",
    "    d_model = Q.shape[-1]\n",
    "    depth = d_model // num_heads\n",
    "\n",
    "    # Divise en plusieurs têtes\n",
    "    Q = split_heads(Q, num_heads)\n",
    "    K = split_heads(K, num_heads)\n",
    "    V = split_heads(V, num_heads)\n",
    "\n",
    "    # Calcul de l'attention pour chaque tête\n",
    "    QK = tf.matmul(Q, K, transpose_b=True)\n",
    "    QK_normalized = QK / tf.math.sqrt(tf.cast(depth, tf.float32))\n",
    "    softmax_weights = tf.nn.softmax(QK_normalized, axis=-1)\n",
    "    attention = tf.matmul(softmax_weights, V)\n",
    "\n",
    "    # Concatène les têtes et applique une transformation linéaire\n",
    "    attention_concatenated = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "    concatenated = tf.reshape(attention_concatenated, (tf.shape(attention)[0], -1, d_model))\n",
    "    return tf.keras.layers.Dense(d_model)(concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d182de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de l'embedding de la phrase: (1, 3, 256)\n"
     ]
    }
   ],
   "source": [
    "# Initialisation\n",
    "sentence = \"Le ciel est\"\n",
    "embedding_layer = tf.keras.layers.Embedding(5000, 256)  \n",
    "tokenized_sentence = [10, 100, 150]\n",
    "embedded_sentence = embedding_layer(tf.convert_to_tensor([tokenized_sentence]))\n",
    "print(\"Dimension de l'embedding de la phrase:\", embedded_sentence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c491194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encodeur:\n",
      "Dimension après multi-head attention: (1, 3, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encodeur avec multi-head attention\n",
    "## Calcul des Query, Key, Value\n",
    "Q_enc = tf.keras.layers.Dense(256)(embedded_sentence)\n",
    "K_enc = tf.keras.layers.Dense(256)(embedded_sentence)\n",
    "V_enc = tf.keras.layers.Dense(256)(embedded_sentence)\n",
    "\n",
    "## Multi-head attention\n",
    "attention_enc = multi_head_attention(Q_enc, K_enc, V_enc, 8)\n",
    "print(\"\\nEncodeur:\")\n",
    "print(\"Dimension après multi-head attention:\", attention_enc.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "389f192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Décodeur:\n",
      "Dimension de l'attention multi-tête (décodeur): (1, 1, 256)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Décodeur avec multi-head attention\n",
    "## Utiliser l'état du dernier mot comme Query\n",
    "Q_dec = Q_enc[:, -1, :]\n",
    "K_dec = K_enc\n",
    "V_dec = V_enc\n",
    "\n",
    "## Multi-head attention\n",
    "attention_dec = multi_head_attention(tf.expand_dims(Q_dec, 1), K_dec, V_dec, 8)\n",
    "print(\"\\nDécodeur:\")\n",
    "print(\"Dimension de l'attention multi-tête (décodeur):\", attention_dec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b02daf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dimension de la sortie: (1, 1, 5000)\n",
      "Index du mot prédit: [[3390]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sortie: prédiction du prochain mot\n",
    "output = tf.keras.layers.Dense(5000, activation='softmax')(attention_dec)  \n",
    "print(\"\\nDimension de la sortie:\", output.shape)\n",
    "\n",
    "# Prédire le mot avec l'indice le plus élevé comme mot suivant\n",
    "predicted_next_word_index = tf.argmax(output, axis=-1)\n",
    "print(\"Index du mot prédit:\", predicted_next_word_index.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
