{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be94e6d6-4096-4d1a-aa58-5afd89f33bff",
   "metadata": {},
   "source": [
    "# Fine-tuning Sandbox\n",
    "\n",
    "Ce notebook contient des exemples de code pour le fine-tuning de grands modèles de langage (Large Language Models, LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5dc7bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (0.6.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from peft) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from peft) (22.0)\n",
      "Requirement already satisfied: psutil in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from peft) (2.1.0)\n",
      "Requirement already satisfied: transformers in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from peft) (4.36.0.dev0)\n",
      "Requirement already satisfied: tqdm in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from peft) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from peft) (0.24.1)\n",
      "Requirement already satisfied: safetensors in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from peft) (0.4.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from accelerate>=0.21.0->peft) (0.17.3)\n",
      "Requirement already satisfied: filelock in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers->peft) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers->peft) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from transformers->peft) (0.14.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from requests->transformers->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from requests->transformers->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from requests->transformers->peft) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch>=1.13.0->peft) (1.2.1)\n",
      "Requirement already satisfied: evaluate in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from evaluate) (2.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: dill in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from evaluate) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from evaluate) (0.17.3)\n",
      "Requirement already satisfied: packaging in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from evaluate) (22.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from pandas->evaluate) (2022.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/natachanjongwayepnga/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Installation des bibliothèques nécessaires\n",
    "!pip install peft\n",
    "!pip install evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df41fa",
   "metadata": {},
   "source": [
    "## Importation des Bibliothèques\n",
    "\n",
    "Importation des bibliothèques nécessaires pour le traitement des données et l'entraînement du modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef8ea85-d04d-4217-99a3-21c446bf2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques essentielles pour le traitement des données et le machine learning\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Bibliothèque pour l'évaluation des modèles\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a4484-07d8-49dd-81ef-672105f53ebe",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa9722d3-0609-4aea-9585-9aa2cfc1fc9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chemin le fichier\n",
    "file_path = 'IMDB_Dataset.csv'\n",
    "\n",
    "# Charger les données depuis le fichier CSV\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe4a8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  One of the other reviewers has mentioned that ...      1\n",
       "1  A wonderful little production. <br /><br />The...      1\n",
       "2  I thought this was a wonderful way to spend ti...      1\n",
       "3  Basically there's a family where a little boy ...      0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversion des étiquettes en format numérique\n",
    "df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "df = df.drop(['sentiment'], axis=1) \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a787ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70c4f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sélectionner un sous-ensemble aléatoire de N échantillons pour l'entraînement et le test\n",
    "N = 1000\n",
    "train_subset = train_df.sample(n=N, random_state=42)\n",
    "test_subset = test_df.sample(n=N, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8df31fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Créer les datasets Hugging Face\n",
    "train_dataset = Dataset.from_pandas(train_subset)\n",
    "test_dataset = Dataset.from_pandas(test_subset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc98dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review', 'label', '__index_level_0__'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf2def6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['review', 'label', '__index_level_0__'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb0564",
   "metadata": {},
   "source": [
    "# Finetuning complet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9c492",
   "metadata": {},
   "source": [
    "DistilBERT (de \"Distilled BERT\") est une version plus petite et plus rapide du modèle BERT (Bidirectional Encoder Representations from Transformers) développé par Hugging Face. Il offre des performances proches de BERT avec une taille réduite et une vitesse accrue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f1ad489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Importe PyTorch, une bibliothèque pour les réseaux de neurones profonds et l'apprentissage automatique.\n",
    "\n",
    "import pandas as pd\n",
    "# Importe pandas, une bibliothèque pour la manipulation et l'analyse de données, souvent utilisée pour manipuler des tableaux de données.\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "# Importe 'Dataset' et 'DatasetDict' de la bibliothèque 'datasets' de Hugging Face, utilisés pour gérer et préparer des ensembles de données pour l'entraînement de modèles.\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "# Importe plusieurs classes de la bibliothèque 'transformers' de Hugging Face:\n",
    "# - DistilBertTokenizer : pour convertir du texte en tokens compréhensibles par DistilBERT.\n",
    "# - DistilBertForSequenceClassification : une version de DistilBERT préparée pour la classification de séquences (par exemple, classification de texte).\n",
    "# - TrainingArguments : pour configurer les paramètres d'entraînement.\n",
    "# - Trainer : pour orchestrer le processus d'entraînement du modèle.\n",
    "\n",
    "import torch.optim as optim\n",
    "# Importe le sous-module 'optim' de PyTorch, qui contient divers algorithmes d'optimisation utilisés pour mettre à jour les poids du réseau pendant l'entraînement, tels que SGD, Adam, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4787ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres d'entraînement\n",
    "model_checkpoint = \"distilbert-base-uncased\"  # Modèle distilbert pré-entraîné\n",
    "lr = 2e-5  # Taux d'apprentissage\n",
    "batch_size = 8  # Taille du lot\n",
    "num_epochs = 3  # Nombre d'époques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ddff9",
   "metadata": {},
   "source": [
    "Dans le domaine du traitement du langage naturel (NLP), le \"padding\" joue un rôle crucial dans la préparation des données pour l'entraînement de modèles. Voici quelques détails clés :\n",
    "\n",
    "Concept de Padding en NLP\n",
    "Uniformité des Longueurs de Séquences : Les modèles de NLP, en particulier ceux basés sur des réseaux de neurones, nécessitent que toutes les entrées (phrases, paragraphes, etc.) dans un lot de données (batch) aient la même longueur pour un traitement efficace. Cependant, dans la réalité, les séquences de mots varient en longueur.\n",
    "Ajout de Valeurs de Remplissage : Pour résoudre ce problème, le padding est utilisé pour \"remplir\" les séquences plus courtes avec des valeurs neutres (souvent des zéros ou un token spécifique) pour qu'elles correspondent à la longueur de la séquence la plus longue dans le lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "877e4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le tokenizer et le modèle pré-entraîné\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "train_encodings = tokenizer(train_dataset['review'], truncation=True, padding=True, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_dataset['review'], truncation=True, padding=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bad96157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 2 : Création des datasets PyTorch\n",
    "# Étape 2 : Création des datasets PyTorch\n",
    "train_datasetb = Dataset.from_dict({\n",
    "    # 'input_ids' : Contient les identifiants de tokens obtenus après le processus de tokenisation du texte. \n",
    "    # Ces identifiants sont nécessaires pour que le modèle comprenne et traite le texte.\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "\n",
    "    # 'attention_mask' : Un masque d'attention qui indique au modèle quels tokens dans 'input_ids' sont significatifs \n",
    "    # (c'est-à-dire non-padding) et doivent être pris en compte lors du traitement.\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "\n",
    "    # 'labels' : Les étiquettes associées à chaque exemple d'entraînement. Ces étiquettes sont utilisées pour l'apprentissage supervisé,\n",
    "    # où le modèle apprend à associer les entrées à ces étiquettes correctes.\n",
    "    # 'torch.tensor()' est utilisé pour convertir la liste des étiquettes en tensor PyTorch, \n",
    "    # ce qui est nécessaire pour le traitement avec PyTorch.\n",
    "    'labels': torch.tensor(train_dataset['label'])\n",
    "})\n",
    "\n",
    "test_datasetb = Dataset.from_dict({\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask'],\n",
    "    'labels': torch.tensor(test_dataset['label'])\n",
    "})\n",
    "\n",
    "# Création d'un DatasetDict\n",
    "datasetb = DatasetDict({\n",
    "    'train': train_datasetb,\n",
    "    'test': test_datasetb\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "307d5fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Création d'un modèle de classification de séquences basé sur DistilBERT\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "634e17b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètres du modèle : 66955010\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Affichage du nombre de paramètres du modèle\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Nombre de paramètres du modèle : {num_params}\")\n",
    "\n",
    "# Afficher l'architecture du modèle BERT\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fd39bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 4 : Définition des paramètres d'entraînement\n",
    "# Configuration de l'optimiseur pour l'entraînement du modèle\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "# Définition de la fonction de perte pour l'entraînement du modèle\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ae0e331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la métrique pour l'évaluation\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b671079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy_value = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    return {\"accuracy\": accuracy_value}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "888eee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Définir la correspondance entre les identifiants de classe et les étiquettes\n",
    "id2label = {0: \"Négatif\", 1: \"Positif\"}\n",
    "label2id = {\"Négatif\": 0, \"Positif\": 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d721148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédictions du modèle non entraîné:\n",
      "-------------------------------------\n",
      "This movie was a masterpiece. - Positif\n",
      "I was blown away by the acting. - Positif\n",
      "It's a classic that everyone should watch. - Positif\n",
      "The plot was confusing and hard to follow. - Négatif\n",
      "The special effects were top-notch. - Négatif\n",
      "I couldn't stop laughing throughout the movie. - Positif\n",
      "The soundtrack was incredible. - Négatif\n",
      "It's a total waste of time. - Positif\n",
      "I'm still thinking about that ending. - Positif\n",
      "I wouldn't recommend it to anyone. - Positif\n"
     ]
    }
   ],
   "source": [
    "liste_texte = [\n",
    "    \"This movie was a masterpiece.\",\n",
    "    \"I was blown away by the acting.\",\n",
    "    \"It's a classic that everyone should watch.\",\n",
    "    \"The plot was confusing and hard to follow.\",\n",
    "    \"The special effects were top-notch.\",\n",
    "    \"I couldn't stop laughing throughout the movie.\",\n",
    "    \"The soundtrack was incredible.\",\n",
    "    \"It's a total waste of time.\",\n",
    "    \"I'm still thinking about that ending.\",\n",
    "    \"I wouldn't recommend it to anyone.\"\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Prédictions du modèle non entraîné:\")\n",
    "print(\"-------------------------------------\")\n",
    "for texte in liste_texte:\n",
    "    # Tokenizer le texte\n",
    "    inputs = tokenizer.encode(texte, return_tensors=\"pt\")\n",
    "    # Calculer les logits\n",
    "    logits = model(inputs).logits\n",
    "    # Convertir les logits en étiquette\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(texte + \" - \" + id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94e1c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 5 : Définition des arguments d'entraînement avec Transformers\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_checkpoint + \"-fullfinetuned-text-classification\",  # Le répertoire où les résultats de l'entraînement seront sauvegardés\n",
    "    learning_rate=lr,  # Taux d'apprentissage pour l'optimisation du modèle\n",
    "    per_device_train_batch_size=batch_size,  # Taille du lot d'entraînement par périphérique (GPU ou CPU)\n",
    "    per_device_eval_batch_size=batch_size,  # Taille du lot d'évaluation par périphérique (GPU ou CPU)\n",
    "    num_train_epochs=num_epochs,  # Nombre d'époques d'entraînement (combien de fois le modèle parcourt l'ensemble de données)\n",
    "    weight_decay=0.01,  # Terme de régularisation pour contrôler le poids de la pénalisation dans la fonction de perte\n",
    "    evaluation_strategy=\"epoch\",  # Stratégie d'évaluation, ici \"epoch\" signifie évaluer à la fin de chaque époque\n",
    "    save_strategy=\"epoch\",  # Stratégie de sauvegarde du modèle, ici \"epoch\" signifie sauvegarder le modèle à la fin de chaque époque\n",
    "    load_best_model_at_end=True  # Charger le meilleur modèle à la fin de l'entraînement\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b680bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 6 : Entraînement du modèle avec Trainer de Transformers\n",
    "trainerfull = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=datasetb['train'],\n",
    "    eval_dataset=datasetb['test'],\n",
    "    compute_metrics=compute_metrics  # Utiliser la fonction de calcul des métriques\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86c0519b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 04:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.309968</td>\n",
       "      <td>0.891000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.365536</td>\n",
       "      <td>0.898000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.402130</td>\n",
       "      <td>0.901000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.22651810709635417, metrics={'train_runtime': 275.285, 'train_samples_per_second': 10.898, 'train_steps_per_second': 1.362, 'total_flos': 397402195968000.0, 'train_loss': 0.22651810709635417, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "trainerfull.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fabbc734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sauvegarde du modèle\n",
    "trainerfull.save_model(\"trainer_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3949a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédictions du modèle entraîné :\n",
      "---------------------------------\n",
      "This movie was a masterpiece. - Positif\n",
      "I was blown away by the acting. - Négatif\n",
      "It's a classic that everyone should watch. - Positif\n",
      "The plot was confusing and hard to follow. - Négatif\n",
      "The special effects were top-notch. - Positif\n",
      "I couldn't stop laughing throughout the movie. - Négatif\n",
      "The soundtrack was incredible. - Positif\n",
      "It's a total waste of time. - Négatif\n",
      "I'm still thinking about that ending. - Négatif\n",
      "I wouldn't recommend it to anyone. - Négatif\n"
     ]
    }
   ],
   "source": [
    "# Passer le modèle en mode 'mps' (pour Mac, vous pouvez également utiliser 'cpu')\n",
    "model.to('mps')\n",
    "\n",
    "# Afficher les prédictions du modèle entraîné\n",
    "print(\"Prédictions du modèle entraîné :\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Parcourir la liste des textes à prédire\n",
    "for texte in liste_texte:\n",
    "    # Tokenizer le texte\n",
    "    inputs = tokenizer.encode(texte, return_tensors=\"pt\").to(\"mps\") # Passage en mode 'mps' (ou 'cpu' en alternative pour Mac)\n",
    "\n",
    "    # Obtenir les logits du modèle pour le texte\n",
    "    logits = model(inputs).logits\n",
    "\n",
    "    # Prédire la classe en choisissant l'indice du logit le plus élevé\n",
    "    predictions = torch.max(logits, 1).indices\n",
    "\n",
    "    # Afficher le texte et l'étiquette prédite\n",
    "    print(texte + \" - \" + id2label[predictions.tolist()[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdac0aa",
   "metadata": {},
   "source": [
    "# Layer freezing finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd1436db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le tokenizer et le modèle pré-entraîné\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "train_encodings = tokenizer(train_dataset['review'], truncation=True, padding=True, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_dataset['review'], truncation=True, padding=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b155767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasetb = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'labels': torch.tensor(train_dataset['label'])\n",
    "})\n",
    "\n",
    "test_datasetb = Dataset.from_dict({\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask'],\n",
    "    'labels': torch.tensor(test_dataset['label'])\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "debad335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Modèle DistilBERT pour la classification de séquences\n",
    "modelf = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01570fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geler certaines couches du modèle\n",
    "for param in modelf.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eec39474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les arguments d'entraînement pour le Trainer de Hugging Face\n",
    "training_args = TrainingArguments(\n",
    "    # Répertoire où les résultats d'entraînement (modèles, configurations, etc.) seront sauvegardés.\n",
    "    output_dir=\"./freezdistilbert_finetuned\",\n",
    "\n",
    "    # Nombre d'exemples par lot (batch) pour l'entraînement, défini pour chaque périphérique (GPU/CPU).\n",
    "    per_device_train_batch_size=8,\n",
    "\n",
    "    # Nombre d'exemples par lot pour l'évaluation, défini pour chaque périphérique.\n",
    "    per_device_eval_batch_size=8,\n",
    "\n",
    "    # Stratégie d'évaluation à utiliser. Ici, \"steps\" signifie que l'évaluation aura lieu à intervalles réguliers de pas d'entraînement.\n",
    "    evaluation_strategy=\"steps\",\n",
    "\n",
    "    # Nombre de pas d'entraînement à effectuer avant de réaliser une évaluation.\n",
    "    eval_steps=100,\n",
    "\n",
    "    # Nombre de pas d'entraînement après lesquels le modèle sera sauvegardé.\n",
    "    save_steps=100,\n",
    "\n",
    "    # Nombre total d'époques d'entraînement à réaliser.\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    # Taux d'apprentissage initial à utiliser pour l'optimiseur.\n",
    "    learning_rate=2e-5,\n",
    "\n",
    "    # Limite du nombre total de sauvegardes de checkpoints à conserver. \n",
    "    # Ici, seul le checkpoint le plus récent sera conservé.\n",
    "    save_total_limit=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0d3c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer la métrique d'exactitude pour l'évaluation\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71f782d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer l'exactitude\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calcul de l'exactitude\n",
    "    accuracy = np.mean(predictions == labels)\n",
    "\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5c978d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding  # Assurez-vous d'avoir cette ligne d'importation\n",
    "\n",
    "\n",
    "# Créer un Trainer\n",
    "trainerfreezing = Trainer(\n",
    "    model=modelf,  # Utilisez le modèle DistilBERT que vous avez défini précédemment\n",
    "    args=training_args,  # Les paramètres d'entraînement définis précédemment\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # Gère le padding des données lors de l'entraînement\n",
    "    compute_metrics=compute_metrics,  # Fonction pour calculer les métriques d'évaluation\n",
    "    train_dataset=train_datasetb,  # L'ensemble de données d'entraînement\n",
    "    eval_dataset=test_datasetb  # L'ensemble de données d'évaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03cc88f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 04:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.322988</td>\n",
       "      <td>0.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.358135</td>\n",
       "      <td>0.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.389590</td>\n",
       "      <td>0.898000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=375, training_loss=0.24811503092447917, metrics={'train_runtime': 274.1337, 'train_samples_per_second': 10.944, 'train_steps_per_second': 1.368, 'total_flos': 397402195968000.0, 'train_loss': 0.24811503092447917, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraîner le modèle\n",
    "trainerfreezing.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "882fcfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédictions du modèle entraîné :\n",
      "---------------------------------\n",
      "This movie was a masterpiece. - Positif\n",
      "I was blown away by the acting. - Négatif\n",
      "It's a classic that everyone should watch. - Positif\n",
      "The plot was confusing and hard to follow. - Négatif\n",
      "The special effects were top-notch. - Positif\n",
      "I couldn't stop laughing throughout the movie. - Négatif\n",
      "The soundtrack was incredible. - Positif\n",
      "It's a total waste of time. - Négatif\n",
      "I'm still thinking about that ending. - Négatif\n",
      "I wouldn't recommend it to anyone. - Négatif\n"
     ]
    }
   ],
   "source": [
    "# Passer le modèle en mode 'mps' (pour Mac, vous pouvez également utiliser 'cpu')\n",
    "modelf.to('mps')\n",
    "\n",
    "# Afficher les prédictions du modèle entraîné\n",
    "print(\"Prédictions du modèle entraîné :\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Parcourir la liste des textes à prédire\n",
    "for texte in liste_texte:\n",
    "    # Tokenizer le texte\n",
    "    inputs = tokenizer.encode(texte, return_tensors=\"pt\").to(\"mps\") # Passage en mode 'mps' (ou 'cpu' en alternative pour Mac)\n",
    "\n",
    "    # Obtenir les logits du modèle pour le texte\n",
    "    logits = modelf(inputs).logits\n",
    "\n",
    "    # Prédire la classe en choisissant l'indice du logit le plus élevé\n",
    "    predictions = torch.max(logits, 1).indices\n",
    "\n",
    "    # Afficher le texte et l'étiquette prédite\n",
    "    \n",
    "    # Afficher le texte et l'étiquette prédite\n",
    "    print(texte + \" - \" + id2label[predictions.tolist()[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644c68d-9adf-48a4-90a2-8fd89555a302",
   "metadata": {},
   "source": [
    "# Finetuning LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5f4758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des outils spécifiques aux modèles transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,  # Pour le tokenizing des textes\n",
    "    AutoModelForSequenceClassification,  # Pour charger un modèle pré-entraîné pour la classification de séquences\n",
    "    TrainingArguments,  # Pour définir les paramètres d'entraînement\n",
    "    Trainer,  # Pour entraîner le modèle\n",
    "    DataCollatorWithPadding  # Pour gérer le padding des séquences lors de l'entraînement\n",
    ")\n",
    "# Importation des bibliothèques pour le fine-tuning avec PEFT (Parameter-Efficient Fine-tuning)\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a60dd1fe-8144-4678-b018-20891e49237a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Définir le modèle que vous souhaitez utiliser pour la classification de séquences. Vous pouvez choisir un modèle pré-entraîné, comme 'distilbert-base-uncased' ou 'roberta-base'.\n",
    "\n",
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "# Générer un modèle de classification à partir du modèle pré-entraîné\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "853002f8-d39c-4bc4-8d07-e44a47de3b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display architecture\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319dc21",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a34f3943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un tokenizer pour le modèle\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "tokenizer.pad_token\n",
    "# Ajouter un jeton de padding s'il n'existe pas\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20f4adb9-ce8f-4f54-9b94-300c9daae1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une fonction pour tokenizer les données\n",
    "def tokenize_function(examples):\n",
    "    text = examples[\"review\"]\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        padding=True,  # Ajout du padding directement ici\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b7600bcd-7e93-4fb4-bd8d-ffc76bed1ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer les ensembles d'entraînement et de test\n",
    "# Créer un DatasetDict\n",
    "# Tokenizer les ensembles d'entraînement et de test et créer un DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f8e85f9-1804-4f49-a783-4da59580ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un DataCollator pour gérer le padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9a120-580d-470c-a981-7c7e22604865",
   "metadata": {},
   "source": [
    "## Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4dde538-cd7f-4ab5-a96d-c30f3003822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du modèle LoRA (Low-Rank Adaptation) pour le fine-tuning\n",
    "\n",
    "# Type de tâche, ici \"SEQ_CLS\" signifie classification de séquences\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",  # Type de tâche du modèle\n",
    "\n",
    "    # Facteur de régularisation \"r\" pour LoRA\n",
    "    r=4,\n",
    "\n",
    "    # Paramètre d'alpha pour LoRA, contrôlant l'importance de la régularisation LoRA\n",
    "    lora_alpha=32,\n",
    "\n",
    "    # Taux de dropout pour LoRA, qui détermine la probabilité de désactivation aléatoire des connexions\n",
    "    lora_dropout=0.01,\n",
    "\n",
    "    # Modules cibles pour l'application de la régularisation LoRA, ici uniquement le module 'q_lin' est ciblé\n",
    "    target_modules=['q_lin']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1391303-1e16-4d5c-b2b4-799997eff9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='SEQ_CLS', inference_mode=False, r=4, target_modules={'q_lin'}, lora_alpha=32, lora_dropout=0.01, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e0d9408-9fc4-4bd3-8d35-4d8217fe01e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9306847223789819\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5db78059-e5ae-4807-89db-b58ef6abedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres\n",
    "\n",
    "# Taux d'apprentissage (learning rate) - C'est la vitesse à laquelle le modèle apprend\n",
    "lr = 1e-3\n",
    "\n",
    "# Taille du lot (batch size) - Le nombre d'exemples de données utilisés pour chaque mise à jour de poids du modèle\n",
    "batch_size = 4\n",
    "\n",
    "# Nombre d'époques (epochs) - Le nombre de fois que le modèle parcourt l'ensemble de données complet lors de l'entraînement\n",
    "num_epochs = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9244ed55-65a4-4c66-8388-55efd87bceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des arguments d'entraînement\n",
    "\n",
    "# Répertoire de sortie où les résultats de l'entraînement seront sauvegardés\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= model_checkpoint + \"-lora-text-classification\",  # Chemin du répertoire de sortie\n",
    "\n",
    "    # Taux d'apprentissage (learning rate) pour l'optimisation du modèle\n",
    "    learning_rate=lr,\n",
    "\n",
    "    # Taille du lot (batch size) d'entraînement par périphérique (GPU ou CPU)\n",
    "    per_device_train_batch_size=batch_size,\n",
    "\n",
    "    # Taille du lot (batch size) d'évaluation par périphérique (GPU ou CPU)\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "\n",
    "    # Nombre d'époques d'entraînement (combien de fois le modèle parcourt l'ensemble de données)\n",
    "    num_train_epochs=num_epochs,\n",
    "\n",
    "    # Terme de régularisation pour contrôler le poids de la pénalisation dans la fonction de perte\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # Stratégie d'évaluation, ici \"epoch\" signifie évaluer à la fin de chaque époque\n",
    "    evaluation_strategy=\"epoch\",\n",
    "\n",
    "    # Stratégie de sauvegarde du modèle, ici \"epoch\" signifie sauvegarder le modèle à la fin de chaque époque\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    # Charger le meilleur modèle à la fin de l'entraînement\n",
    "    load_best_model_at_end=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc8bc705-5dd7-4305-a797-399b2b0fa2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'objet \"trainer\" pour entraîner le modèle\n",
    "\n",
    "# Le modèle à entraîner\n",
    "trainer = Trainer(\n",
    "    model=model,  # Le modèle que vous avez configuré précédemment\n",
    "\n",
    "    # Les arguments d'entraînement définis précédemment\n",
    "    args=training_args,\n",
    "\n",
    "    # L'ensemble de données d'entraînement tokenisé\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "\n",
    "    # L'ensemble de données de validation tokenisé\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "\n",
    "    # Le tokenizer utilisé pour le prétraitement\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    # Le \"data_collator\" qui gère le padding dynamique des exemples dans chaque lot pour qu'ils aient la même longueur\n",
    "    data_collator=data_collator,\n",
    "\n",
    "    # La fonction de calcul des métriques pour évaluer le modèle\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "454275e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 03:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.429260</td>\n",
       "      <td>0.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.585174</td>\n",
       "      <td>0.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.702090</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.2660241495768229, metrics={'train_runtime': 230.0579, 'train_samples_per_second': 13.04, 'train_steps_per_second': 3.26, 'total_flos': 403199004672000.0, 'train_loss': 0.2660241495768229, 'epoch': 3.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entraînement du modèle\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5664d1-9bd2-4ce1-bc24-cab5adf80f49",
   "metadata": {},
   "source": [
    "### Generate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e5dc029e-1c16-491d-a3f1-715f9e0adf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédictions du modèle entraîné :\n",
      "---------------------------------\n",
      "This movie was a masterpiece. - Positif\n",
      "I was blown away by the acting. - Négatif\n",
      "It's a classic that everyone should watch. - Positif\n",
      "The plot was confusing and hard to follow. - Négatif\n",
      "The special effects were top-notch. - Positif\n",
      "I couldn't stop laughing throughout the movie. - Négatif\n",
      "The soundtrack was incredible. - Positif\n",
      "It's a total waste of time. - Négatif\n",
      "I'm still thinking about that ending. - Négatif\n",
      "I wouldn't recommend it to anyone. - Négatif\n"
     ]
    }
   ],
   "source": [
    "# Passer le modèle en mode 'mps' (pour Mac, vous pouvez également utiliser 'cpu')\n",
    "model.to('mps')\n",
    "\n",
    "# Afficher les prédictions du modèle entraîné\n",
    "print(\"Prédictions du modèle entraîné :\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Parcourir la liste des textes à prédire\n",
    "for texte in liste_texte:\n",
    "    # Tokenizer le texte\n",
    "    inputs = tokenizer.encode(texte, return_tensors=\"pt\").to(\"mps\") # Passage en mode 'mps' (ou 'cpu' en alternative pour Mac)\n",
    "\n",
    "    # Obtenir les logits du modèle pour le texte\n",
    "    logits = model(inputs).logits\n",
    "\n",
    "    # Prédire la classe en choisissant l'indice du logit le plus élevé\n",
    "    predictions = torch.max(logits, 1).indices\n",
    "\n",
    "    # Afficher le texte et l'étiquette prédite\n",
    "    print(texte + \" - \" + id2label[predictions.tolist()[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb251c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
